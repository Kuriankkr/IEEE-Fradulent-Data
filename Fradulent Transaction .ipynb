{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_columns = 10000\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = \"I:/ML Datasets/IEEE Fradulent/train_identity.csv\"\n",
    "train_identity = pd.read_csv(path)\n",
    "\n",
    "path = \"I:/ML Datasets/IEEE Fradulent/train_transaction.csv\"\n",
    "train_transaction = pd.read_csv(path)\n",
    "\n",
    "path = \"I:/ML Datasets/IEEE Fradulent/test_identity.csv\"\n",
    "test_identity= pd.read_csv(path)\n",
    "\n",
    "path = \"I:/ML Datasets/IEEE Fradulent/test_transaction.csv\"\n",
    "test_transaction = pd.read_csv(path)\n",
    "\n",
    "train = train_transaction.merge(train_identity, how='left', on='TransactionID')\n",
    "test = test_transaction.merge(test_identity, how='left', on='TransactionID')\n",
    "\n",
    "#path = \"/content/drive/My Drive/Dataset/IEEE Fradulent/sample_submission.csv\"\n",
    "#df_submission = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccols = [f'C{i}' for i in range(1,15)] ## Creating columns C1,C2....C14\n",
    "dcols = [f'D{i}' for i in range(1,16)] ## Creating columns D1,D2....D14\n",
    "\n",
    "cols = [\n",
    "    'TransactionID','isFraud','TransactionDT','TransactionAmt','ProductCD',\n",
    "    'card1','card2','card3','card4','card5','card6',\n",
    "    'addr1','addr2','dist1','dist2',\n",
    "    'P_emaildomain','R_emaildomain',\n",
    "    \n",
    "]\n",
    "\n",
    "cols += dcols\n",
    "cols += ccols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1        3\n",
       "C2        3\n",
       "C3        3\n",
       "C4        3\n",
       "C5        3\n",
       "C6        3\n",
       "C7        3\n",
       "C8        3\n",
       "C9        3\n",
       "C10       3\n",
       "C11       3\n",
       "C12       3\n",
       "C13    4748\n",
       "C14       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[ccols].isna().sum()  ## Finding all those C columns that have Na values and replacing them with 0's//\n",
    "#Note: the train columns (C1-C14) have no Nan values and therefore they dont have to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1     0\n",
       "C2     0\n",
       "C3     0\n",
       "C4     0\n",
       "C5     0\n",
       "C6     0\n",
       "C7     0\n",
       "C8     0\n",
       "C9     0\n",
       "C10    0\n",
       "C11    0\n",
       "C12    0\n",
       "C13    0\n",
       "C14    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[ccols] = test[ccols].fillna(0)\n",
    "test[ccols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Had to rename these columns because there were differnt in the test and the train columns\n",
    "\n",
    "test =  test.rename(columns = {'id-01':'id_01', 'id-02':'id_02', 'id-03':'id_03', 'id-04':'id_04', 'id-05':'id_05', 'id-06':'id_06','id-07':'id_07','id-08':'id_08',\n",
    "     'id-09':'id_09','id-10':'id_10','id-11':'id_11', 'id-12':'id_12', 'id-13':'id_13', 'id-14':'id_14', 'id-15':'id_15', 'id-16':'id_16',\n",
    "       'id-17':'id_17', 'id-18':'id_18','id-19':'id_19', 'id-20':'id_20', 'id-21':'id_21','id-22':'id_22','id-23':'id_23', 'id-24':'id_24',\n",
    "      'id-25':'id_25', 'id-26':'id_26', 'id-27':'id_27', 'id-28':'id_28', 'id-29':'id_29','id-30':'id_30','id-31':'id_31','id-32':'id_32',\n",
    "      'id-33':'id_33','id-34':'id_34', 'id-35':'id_35','id-36':'id_36','id-37':'id_37','id-38':'id_38'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1097231, 434)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['isFraud'] = 0\n",
    "#test = test[train.columns]\n",
    "\n",
    "train_test = pd.concat([train, test], axis=0)\n",
    "print(train_test.shape)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As to the reason why we are using this function to reduce memory usage there is a briliiant article on this in the website\n",
    "https://www.kaggle.com/alexeykupershtokh/safe-memory-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe downcast\n",
    "def sd(col, max_loss_limit=0.001, avg_loss_limit=0.001, na_loss_limit=0, n_uniq_loss_limit=0, fillna=0):\n",
    "    \"\"\"\n",
    "    max_loss_limit - don't allow any float to lose precision more than this value. Any values are ok for GBT algorithms as long as you don't unique values.\n",
    "                     See https://en.wikipedia.org/wiki/Half-precision_floating-point_format#Precision_limitations_on_decimal_values_in_[0,_1]\n",
    "    avg_loss_limit - same but calculates avg throughout the series.\n",
    "    na_loss_limit - not really useful.\n",
    "    n_uniq_loss_limit - very important parameter. If you have a float field with very high cardinality(high number of unique\n",
    "    values) you can set this value to something like n_records * 0.01 in order to allow some field relaxing.\n",
    "    \"\"\"\n",
    "    is_float = str(col.dtypes)[:5] == 'float'\n",
    "    na_count = col.isna().sum()\n",
    "    n_uniq = col.nunique(dropna=False)\n",
    "    try_types = ['float16', 'float32']\n",
    "\n",
    "    if na_count <= na_loss_limit:\n",
    "        try_types = ['int8', 'int16', 'float16', 'int32', 'float32']\n",
    "\n",
    "    for type in try_types:\n",
    "        col_tmp = col\n",
    "\n",
    "        # float to int conversion => try to round to minimize casting error\n",
    "        if is_float and (str(type)[:3] == 'int'):\n",
    "            col_tmp = col_tmp.copy().fillna(fillna).round()\n",
    "\n",
    "        col_tmp = col_tmp.astype(type)\n",
    "        max_loss = (col_tmp - col).abs().max()\n",
    "        avg_loss = (col_tmp - col).abs().mean()\n",
    "        na_loss = np.abs(na_count - col_tmp.isna().sum())\n",
    "        n_uniq_loss = np.abs(n_uniq - col_tmp.nunique(dropna=False))\n",
    "\n",
    "        if max_loss <= max_loss_limit and avg_loss <= avg_loss_limit and na_loss <= na_loss_limit and n_uniq_loss <= n_uniq_loss_limit:\n",
    "            return col_tmp\n",
    "\n",
    "    # field can't be converted\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage_sd(df, deep=True, verbose=False, obj_to_cat=False):\n",
    "    numerics = ['int16', 'uint16', 'int32', 'uint32', 'int64', 'uint64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=deep).sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        # collect stats\n",
    "        na_count = df[col].isna().sum()\n",
    "        n_uniq = df[col].nunique(dropna=False)\n",
    "        \n",
    "        # numerics\n",
    "        if col_type in numerics:\n",
    "            df[col] = sd(df[col])\n",
    "\n",
    "        # strings\n",
    "        if (col_type == 'object') and obj_to_cat:\n",
    "            df[col] = df[col].astype('category')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Column {col}: {col_type} -> {df[col].dtypes}, na_count={na_count}, n_uniq={n_uniq}')\n",
    "        new_na_count = df[col].isna().sum()\n",
    "        if (na_count != new_na_count):\n",
    "            print(f'Warning: column {col}, {col_type} -> {df[col].dtypes} lost na values. Before: {na_count}, after: {new_na_count}')\n",
    "        new_n_uniq = df[col].nunique(dropna=False)\n",
    "        if (n_uniq != new_n_uniq):\n",
    "            print(f'Warning: column {col}, {col_type} -> {df[col].dtypes} lost unique values. Before: {n_uniq}, after: {new_n_uniq}')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=deep).sum() / 1024 ** 2\n",
    "    percent = 100 * (start_mem - end_mem) / start_mem\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column C1: float64 -> int16, na_count=0, n_uniq=2187\n",
      "Column C10: float64 -> int16, na_count=0, n_uniq=1387\n",
      "Column C11: float64 -> int16, na_count=0, n_uniq=1834\n",
      "Column C12: float64 -> int16, na_count=0, n_uniq=1530\n",
      "Column C13: float64 -> int16, na_count=0, n_uniq=1691\n",
      "Column C14: float64 -> int16, na_count=0, n_uniq=1150\n",
      "Column C2: float64 -> int16, na_count=0, n_uniq=1784\n",
      "Column C3: float64 -> int8, na_count=0, n_uniq=32\n",
      "Column C4: float64 -> int16, na_count=0, n_uniq=1532\n",
      "Column C5: float64 -> int16, na_count=0, n_uniq=353\n",
      "Column C6: float64 -> int16, na_count=0, n_uniq=1574\n",
      "Column C7: float64 -> int16, na_count=0, n_uniq=1335\n",
      "Column C8: float64 -> int16, na_count=0, n_uniq=1419\n",
      "Column C9: float64 -> int16, na_count=0, n_uniq=360\n",
      "Column D1: float64 -> float16, na_count=7300, n_uniq=643\n",
      "Column D10: float64 -> float16, na_count=88567, n_uniq=1076\n",
      "Column D11: float64 -> float16, na_count=455805, n_uniq=888\n",
      "Column D12: float64 -> float16, na_count=963260, n_uniq=869\n",
      "Column D13: float64 -> float16, na_count=911895, n_uniq=828\n",
      "Column D14: float64 -> float16, na_count=919850, n_uniq=1037\n",
      "Column D15: float64 -> float16, na_count=101182, n_uniq=1092\n",
      "Column D2: float64 -> float16, na_count=515566, n_uniq=643\n",
      "Column D3: float64 -> float16, na_count=466020, n_uniq=889\n",
      "Column D4: float64 -> float16, na_count=245773, n_uniq=1070\n",
      "Column D5: float64 -> float16, na_count=534216, n_uniq=962\n",
      "Column D6: float64 -> float16, na_count=899261, n_uniq=1077\n",
      "Column D7: float64 -> float16, na_count=998181, n_uniq=906\n",
      "Column D8: float64 -> float32, na_count=947967, n_uniq=17083\n",
      "Column D9: float64 -> float16, na_count=947967, n_uniq=25\n",
      "Column DeviceInfo: object -> object, na_count=863508, n_uniq=2800\n",
      "Column DeviceType: object -> object, na_count=819490, n_uniq=3\n",
      "Column M1: object -> object, na_count=447739, n_uniq=3\n",
      "Column M2: object -> object, na_count=447739, n_uniq=3\n",
      "Column M3: object -> object, na_count=447739, n_uniq=3\n",
      "Column M4: object -> object, na_count=519189, n_uniq=4\n",
      "Column M5: object -> object, na_count=660114, n_uniq=3\n",
      "Column M6: object -> object, na_count=328299, n_uniq=3\n",
      "Column M7: object -> object, na_count=581283, n_uniq=3\n",
      "Column M8: object -> object, na_count=581256, n_uniq=3\n",
      "Column M9: object -> object, na_count=581256, n_uniq=3\n",
      "Column P_emaildomain: object -> object, na_count=163648, n_uniq=61\n",
      "Column ProductCD: object -> object, na_count=0, n_uniq=5\n",
      "Column R_emaildomain: object -> object, na_count=824070, n_uniq=61\n",
      "Column TransactionAmt: float64 -> float32, na_count=0, n_uniq=29806\n",
      "Column TransactionDT: int64 -> int32, na_count=0, n_uniq=1068035\n",
      "Column TransactionID: int64 -> int32, na_count=0, n_uniq=1097231\n",
      "Column V1: float64 -> float16, na_count=455805, n_uniq=3\n",
      "Column V10: float64 -> float16, na_count=455805, n_uniq=7\n",
      "Column V100: float64 -> float16, na_count=314, n_uniq=32\n",
      "Column V101: float64 -> float16, na_count=314, n_uniq=871\n",
      "Column V102: float64 -> float16, na_count=314, n_uniq=1286\n",
      "Column V103: float64 -> float16, na_count=314, n_uniq=929\n",
      "Column V104: float64 -> float16, na_count=314, n_uniq=61\n",
      "Column V105: float64 -> float16, na_count=314, n_uniq=101\n",
      "Column V106: float64 -> float16, na_count=314, n_uniq=82\n",
      "Column V107: float64 -> float16, na_count=314, n_uniq=3\n",
      "Column V108: float64 -> float16, na_count=314, n_uniq=10\n",
      "Column V109: float64 -> float16, na_count=314, n_uniq=10\n",
      "Column V11: float64 -> float16, na_count=455805, n_uniq=9\n",
      "Column V110: float64 -> float16, na_count=314, n_uniq=10\n",
      "Column V111: float64 -> float16, na_count=314, n_uniq=11\n",
      "Column V112: float64 -> float16, na_count=314, n_uniq=11\n",
      "Column V113: float64 -> float16, na_count=314, n_uniq=11\n",
      "Column V114: float64 -> float16, na_count=314, n_uniq=11\n",
      "Column V115: float64 -> float16, na_count=314, n_uniq=11\n",
      "Column V116: float64 -> float16, na_count=314, n_uniq=11\n",
      "Column V117: float64 -> float16, na_count=314, n_uniq=5\n",
      "Column V118: float64 -> float16, na_count=314, n_uniq=5\n",
      "Column V119: float64 -> float16, na_count=314, n_uniq=5\n",
      "Column V12: float64 -> float16, na_count=88662, n_uniq=6\n",
      "Column V120: float64 -> float16, na_count=314, n_uniq=6\n",
      "Column V121: float64 -> float16, na_count=314, n_uniq=6\n",
      "Column V122: float64 -> float16, na_count=314, n_uniq=6\n",
      "Column V123: float64 -> float16, na_count=314, n_uniq=15\n",
      "Column V124: float64 -> float16, na_count=314, n_uniq=15\n",
      "Column V125: float64 -> float16, na_count=314, n_uniq=15\n",
      "Column V126: float64 -> float32, na_count=314, n_uniq=15136\n",
      "Column V127: float64 -> float32, na_count=314, n_uniq=33547\n",
      "Column V128: float64 -> float32, na_count=314, n_uniq=20511\n",
      "Column V129: float64 -> float32, na_count=314, n_uniq=2639\n",
      "Column V13: float64 -> float16, na_count=88662, n_uniq=8\n",
      "Column V130: float64 -> float32, na_count=314, n_uniq=16251\n",
      "Column V131: float64 -> float32, na_count=314, n_uniq=5790\n",
      "Column V132: float64 -> float32, na_count=314, n_uniq=9304\n",
      "Column V133: float64 -> float32, na_count=314, n_uniq=13669\n",
      "Column V134: float64 -> float32, na_count=314, n_uniq=11315\n",
      "Column V135: float64 -> float32, na_count=314, n_uniq=6276\n",
      "Column V136: float64 -> float32, na_count=314, n_uniq=8208\n",
      "Column V137: float64 -> float32, na_count=314, n_uniq=7139\n",
      "Column V138: float64 -> float16, na_count=939501, n_uniq=24\n",
      "Column V139: float64 -> float16, na_count=939501, n_uniq=35\n",
      "Column V14: float64 -> float16, na_count=88662, n_uniq=3\n",
      "Column V140: float64 -> float16, na_count=939501, n_uniq=61\n",
      "Column V141: float64 -> float16, na_count=939501, n_uniq=8\n",
      "Column V142: float64 -> float16, na_count=939501, n_uniq=13\n",
      "Column V143: float64 -> float16, na_count=939225, n_uniq=871\n",
      "Column V144: float64 -> float16, na_count=939225, n_uniq=87\n",
      "Column V145: float64 -> float16, na_count=939225, n_uniq=299\n",
      "Column V146: float64 -> float16, na_count=939501, n_uniq=26\n",
      "Column V147: float64 -> float16, na_count=939501, n_uniq=28\n",
      "Column V148: float64 -> float16, na_count=939501, n_uniq=22\n",
      "Column V149: float64 -> float16, na_count=939501, n_uniq=22\n",
      "Column V15: float64 -> float16, na_count=88662, n_uniq=14\n",
      "Column V150: float64 -> float32, na_count=939225, n_uniq=3294\n",
      "Column V151: float64 -> float16, na_count=939225, n_uniq=59\n",
      "Column V152: float64 -> float16, na_count=939225, n_uniq=45\n",
      "Column V153: float64 -> float16, na_count=939501, n_uniq=20\n",
      "Column V154: float64 -> float16, na_count=939501, n_uniq=20\n",
      "Column V155: float64 -> float16, na_count=939501, n_uniq=26\n",
      "Column V156: float64 -> float16, na_count=939501, n_uniq=26\n",
      "Column V157: float64 -> float16, na_count=939501, n_uniq=26\n",
      "Column V158: float64 -> float16, na_count=939501, n_uniq=26\n",
      "Column V159: float64 -> float32, na_count=939225, n_uniq=12643\n",
      "Column V16: float64 -> float16, na_count=88662, n_uniq=24\n",
      "Column V160: float64 -> float32, na_count=939225, n_uniq=17183\n",
      "Column V161: float64 -> float32, na_count=939501, n_uniq=106\n",
      "Column V162: float64 -> float32, na_count=939501, n_uniq=290\n",
      "Column V163: float64 -> float32, na_count=939501, n_uniq=154\n",
      "Column V164: float64 -> float32, na_count=939225, n_uniq=2751\n",
      "Column V165: float64 -> float32, na_count=939225, n_uniq=3800\n",
      "Column V166: float64 -> float32, na_count=939225, n_uniq=2262\n",
      "Column V167: float64 -> float16, na_count=820866, n_uniq=874\n",
      "Column V168: float64 -> float16, na_count=820866, n_uniq=966\n",
      "Column V169: float64 -> float16, na_count=821037, n_uniq=41\n",
      "Column V17: float64 -> float16, na_count=88662, n_uniq=17\n",
      "Column V170: float64 -> float16, na_count=821037, n_uniq=65\n",
      "Column V171: float64 -> float16, na_count=821037, n_uniq=70\n",
      "Column V172: float64 -> float16, na_count=820866, n_uniq=39\n",
      "Column V173: float64 -> float16, na_count=820866, n_uniq=11\n",
      "Column V174: float64 -> float16, na_count=821037, n_uniq=10\n",
      "Column V175: float64 -> float16, na_count=821037, n_uniq=24\n",
      "Column V176: float64 -> float16, na_count=820866, n_uniq=241\n",
      "Column V177: float64 -> float16, na_count=820866, n_uniq=863\n",
      "Column V178: float64 -> float16, na_count=820866, n_uniq=1237\n",
      "Column V179: float64 -> float16, na_count=820866, n_uniq=922\n",
      "Column V18: float64 -> float16, na_count=88662, n_uniq=17\n",
      "Column V180: float64 -> float16, na_count=821037, n_uniq=181\n",
      "Column V181: float64 -> float16, na_count=820866, n_uniq=87\n",
      "Column V182: float64 -> float16, na_count=820866, n_uniq=127\n",
      "Column V183: float64 -> float16, na_count=820866, n_uniq=108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column V184: float64 -> float16, na_count=821037, n_uniq=18\n",
      "Column V185: float64 -> float16, na_count=821037, n_uniq=33\n",
      "Column V186: float64 -> float16, na_count=820866, n_uniq=41\n",
      "Column V187: float64 -> float16, na_count=820866, n_uniq=216\n",
      "Column V188: float64 -> float16, na_count=821037, n_uniq=46\n",
      "Column V189: float64 -> float16, na_count=821037, n_uniq=46\n",
      "Column V19: float64 -> float16, na_count=88662, n_uniq=14\n",
      "Column V190: float64 -> float16, na_count=820866, n_uniq=226\n",
      "Column V191: float64 -> float16, na_count=820866, n_uniq=23\n",
      "Column V192: float64 -> float16, na_count=820866, n_uniq=46\n",
      "Column V193: float64 -> float16, na_count=820866, n_uniq=39\n",
      "Column V194: float64 -> float16, na_count=821037, n_uniq=9\n",
      "Column V195: float64 -> float16, na_count=821037, n_uniq=20\n",
      "Column V196: float64 -> float16, na_count=820866, n_uniq=43\n",
      "Column V197: float64 -> float16, na_count=821037, n_uniq=16\n",
      "Column V198: float64 -> float16, na_count=821037, n_uniq=30\n",
      "Column V199: float64 -> float16, na_count=820866, n_uniq=226\n",
      "Column V2: float64 -> float16, na_count=455805, n_uniq=13\n",
      "Column V20: float64 -> float16, na_count=88662, n_uniq=24\n",
      "Column V200: float64 -> float16, na_count=821037, n_uniq=49\n",
      "Column V201: float64 -> float16, na_count=821037, n_uniq=57\n",
      "Column V202: float64 -> float32, na_count=820866, n_uniq=18619\n",
      "Column V203: float64 -> float32, na_count=820866, n_uniq=26291\n",
      "Column V204: float64 -> float32, na_count=820866, n_uniq=22356\n",
      "Column V205: float64 -> float32, na_count=820866, n_uniq=3658\n",
      "Column V206: float64 -> float32, na_count=820866, n_uniq=2968\n",
      "Column V207: float64 -> float32, na_count=820866, n_uniq=5254\n",
      "Column V208: float64 -> float32, na_count=821037, n_uniq=4081\n",
      "Column V209: float64 -> float32, na_count=821037, n_uniq=5927\n",
      "Column V21: float64 -> float16, na_count=88662, n_uniq=7\n",
      "Column V210: float64 -> float32, na_count=821037, n_uniq=4675\n",
      "Column V211: float64 -> float32, na_count=820866, n_uniq=13032\n",
      "Column V212: float64 -> float32, na_count=820866, n_uniq=15640\n",
      "Column V213: float64 -> float32, na_count=820866, n_uniq=14583\n",
      "Column V214: float64 -> float32, na_count=820866, n_uniq=3817\n",
      "Column V215: float64 -> float32, na_count=820866, n_uniq=4665\n",
      "Column V216: float64 -> float32, na_count=820866, n_uniq=4196\n",
      "Column V217: float64 -> float16, na_count=840073, n_uniq=305\n",
      "Column V218: float64 -> float16, na_count=840073, n_uniq=402\n",
      "Column V219: float64 -> float16, na_count=840073, n_uniq=380\n",
      "Column V22: float64 -> float16, na_count=88662, n_uniq=10\n",
      "Column V220: float64 -> float16, na_count=818499, n_uniq=45\n",
      "Column V221: float64 -> float16, na_count=818499, n_uniq=86\n",
      "Column V222: float64 -> float16, na_count=818499, n_uniq=294\n",
      "Column V223: float64 -> float16, na_count=840073, n_uniq=18\n",
      "Column V224: float64 -> float16, na_count=840073, n_uniq=92\n",
      "Column V225: float64 -> float16, na_count=840073, n_uniq=38\n",
      "Column V226: float64 -> float16, na_count=840073, n_uniq=114\n",
      "Column V227: float64 -> float16, na_count=818499, n_uniq=69\n",
      "Column V228: float64 -> float16, na_count=840073, n_uniq=241\n",
      "Column V229: float64 -> float16, na_count=840073, n_uniq=264\n",
      "Column V23: float64 -> float16, na_count=88662, n_uniq=17\n",
      "Column V230: float64 -> float16, na_count=840073, n_uniq=264\n",
      "Column V231: float64 -> float16, na_count=840073, n_uniq=295\n",
      "Column V232: float64 -> float16, na_count=840073, n_uniq=339\n",
      "Column V233: float64 -> float16, na_count=840073, n_uniq=334\n",
      "Column V234: float64 -> float16, na_count=818499, n_uniq=309\n",
      "Column V235: float64 -> float16, na_count=840073, n_uniq=25\n",
      "Column V236: float64 -> float16, na_count=840073, n_uniq=56\n",
      "Column V237: float64 -> float16, na_count=840073, n_uniq=41\n",
      "Column V238: float64 -> float16, na_count=818499, n_uniq=25\n",
      "Column V239: float64 -> float16, na_count=818499, n_uniq=25\n",
      "Column V24: float64 -> float16, na_count=88662, n_uniq=27\n",
      "Column V240: float64 -> float16, na_count=840073, n_uniq=8\n",
      "Column V241: float64 -> float16, na_count=840073, n_uniq=7\n",
      "Column V242: float64 -> float16, na_count=840073, n_uniq=29\n",
      "Column V243: float64 -> float16, na_count=840073, n_uniq=59\n",
      "Column V244: float64 -> float16, na_count=840073, n_uniq=35\n",
      "Column V245: float64 -> float16, na_count=818499, n_uniq=70\n",
      "Column V246: float64 -> float16, na_count=840073, n_uniq=226\n",
      "Column V247: float64 -> float16, na_count=840073, n_uniq=20\n",
      "Column V248: float64 -> float16, na_count=840073, n_uniq=32\n",
      "Column V249: float64 -> float16, na_count=840073, n_uniq=24\n",
      "Column V25: float64 -> float16, na_count=88662, n_uniq=14\n",
      "Column V250: float64 -> float16, na_count=818499, n_uniq=20\n",
      "Column V251: float64 -> float16, na_count=818499, n_uniq=20\n",
      "Column V252: float64 -> float16, na_count=840073, n_uniq=26\n",
      "Column V253: float64 -> float16, na_count=840073, n_uniq=106\n",
      "Column V254: float64 -> float16, na_count=840073, n_uniq=56\n",
      "Column V255: float64 -> float16, na_count=818499, n_uniq=53\n",
      "Column V256: float64 -> float16, na_count=818499, n_uniq=57\n",
      "Column V257: float64 -> float16, na_count=840073, n_uniq=226\n",
      "Column V258: float64 -> float16, na_count=840073, n_uniq=271\n",
      "Column V259: float64 -> float16, na_count=818499, n_uniq=78\n",
      "Column V26: float64 -> float16, na_count=88662, n_uniq=24\n",
      "Column V260: float64 -> float16, na_count=840073, n_uniq=16\n",
      "Column V261: float64 -> float16, na_count=840073, n_uniq=74\n",
      "Column V262: float64 -> float16, na_count=840073, n_uniq=33\n",
      "Column V263: float64 -> float32, na_count=840073, n_uniq=18232\n",
      "Column V264: float64 -> float32, na_count=840073, n_uniq=23850\n",
      "Column V265: float64 -> float32, na_count=840073, n_uniq=20862\n",
      "Column V266: float64 -> float32, na_count=840073, n_uniq=3614\n",
      "Column V267: float64 -> float32, na_count=840073, n_uniq=6124\n",
      "Column V268: float64 -> float32, na_count=840073, n_uniq=4564\n",
      "Column V269: float64 -> float32, na_count=840073, n_uniq=227\n",
      "Column V27: float64 -> float16, na_count=88662, n_uniq=9\n",
      "Column V270: float64 -> float32, na_count=818499, n_uniq=3820\n",
      "Column V271: float64 -> float32, na_count=818499, n_uniq=4798\n",
      "Column V272: float64 -> float32, na_count=818499, n_uniq=4160\n",
      "Column V273: float64 -> float32, na_count=840073, n_uniq=12866\n",
      "Column V274: float64 -> float32, na_count=840073, n_uniq=15112\n",
      "Column V275: float64 -> float32, na_count=840073, n_uniq=14144\n",
      "Column V276: float64 -> float32, na_count=840073, n_uniq=3741\n",
      "Column V277: float64 -> float32, na_count=840073, n_uniq=4209\n",
      "Column V278: float64 -> float32, na_count=840073, n_uniq=3961\n",
      "Column V279: float64 -> float16, na_count=15, n_uniq=882\n",
      "Column V28: float64 -> float16, na_count=88662, n_uniq=9\n",
      "Column V280: float64 -> float16, na_count=15, n_uniq=976\n",
      "Column V281: float64 -> float16, na_count=7300, n_uniq=32\n",
      "Column V282: float64 -> float16, na_count=7300, n_uniq=65\n",
      "Column V283: float64 -> float16, na_count=7300, n_uniq=70\n",
      "Column V284: float64 -> float16, na_count=15, n_uniq=14\n",
      "Column V285: float64 -> float16, na_count=15, n_uniq=97\n",
      "Column V286: float64 -> float16, na_count=15, n_uniq=10\n",
      "Column V287: float64 -> float16, na_count=15, n_uniq=33\n",
      "Column V288: float64 -> float16, na_count=7300, n_uniq=12\n",
      "Column V289: float64 -> float16, na_count=7300, n_uniq=14\n",
      "Column V29: float64 -> float16, na_count=88662, n_uniq=7\n",
      "Column V290: float64 -> float16, na_count=15, n_uniq=62\n",
      "Column V291: float64 -> float16, na_count=15, n_uniq=316\n",
      "Column V292: float64 -> float16, na_count=15, n_uniq=193\n",
      "Column V293: float64 -> float16, na_count=15, n_uniq=871\n",
      "Column V294: float64 -> float16, na_count=15, n_uniq=1287\n",
      "Column V295: float64 -> float16, na_count=15, n_uniq=929\n",
      "Column V296: float64 -> float16, na_count=7300, n_uniq=181\n",
      "Column V297: float64 -> float16, na_count=15, n_uniq=87\n",
      "Column V298: float64 -> float16, na_count=15, n_uniq=127\n",
      "Column V299: float64 -> float16, na_count=15, n_uniq=108\n",
      "Column V3: float64 -> float16, na_count=455805, n_uniq=13\n",
      "Column V30: float64 -> float16, na_count=88662, n_uniq=10\n",
      "Column V300: float64 -> float16, na_count=7300, n_uniq=16\n",
      "Column V301: float64 -> float16, na_count=7300, n_uniq=16\n",
      "Column V302: float64 -> float16, na_count=15, n_uniq=18\n",
      "Column V303: float64 -> float16, na_count=15, n_uniq=22\n",
      "Column V304: float64 -> float16, na_count=15, n_uniq=19\n",
      "Column V305: float64 -> float16, na_count=15, n_uniq=3\n",
      "Column V306: float64 -> float32, na_count=15, n_uniq=25279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column V307: float64 -> float32, na_count=15, n_uniq=54582\n",
      "Column V308: float64 -> float32, na_count=15, n_uniq=35249\n",
      "Column V309: float64 -> float32, na_count=15, n_uniq=6164\n",
      "Column V31: float64 -> float16, na_count=88662, n_uniq=14\n",
      "Column V310: float64 -> float32, na_count=15, n_uniq=26505\n",
      "Column V311: float64 -> float32, na_count=15, n_uniq=4609\n",
      "Column V312: float64 -> float32, na_count=15, n_uniq=11367\n",
      "Column V313: float64 -> float32, na_count=7300, n_uniq=8055\n",
      "Column V314: float64 -> float32, na_count=7300, n_uniq=16327\n",
      "Column V315: float64 -> float32, na_count=7300, n_uniq=10174\n",
      "Column V316: float64 -> float32, na_count=15, n_uniq=14985\n",
      "Column V317: float64 -> float32, na_count=15, n_uniq=22593\n",
      "Column V318: float64 -> float32, na_count=15, n_uniq=18608\n",
      "Column V319: float64 -> float32, na_count=15, n_uniq=8173\n",
      "Column V32: float64 -> float16, na_count=88662, n_uniq=24\n",
      "Column V320: float64 -> float32, na_count=15, n_uniq=10834\n",
      "Column V321: float64 -> float32, na_count=15, n_uniq=9385\n",
      "Column V322: float64 -> float16, na_count=938449, n_uniq=882\n",
      "Column V323: float64 -> float16, na_count=938449, n_uniq=1412\n",
      "Column V324: float64 -> float16, na_count=938449, n_uniq=977\n",
      "Column V325: float64 -> float16, na_count=938449, n_uniq=14\n",
      "Column V326: float64 -> float16, na_count=938449, n_uniq=46\n",
      "Column V327: float64 -> float16, na_count=938449, n_uniq=33\n",
      "Column V328: float64 -> float16, na_count=938449, n_uniq=87\n",
      "Column V329: float64 -> float16, na_count=938449, n_uniq=127\n",
      "Column V33: float64 -> float16, na_count=88662, n_uniq=14\n",
      "Column V330: float64 -> float16, na_count=938449, n_uniq=108\n",
      "Column V331: float64 -> float32, na_count=938449, n_uniq=2093\n",
      "Column V332: float64 -> float32, na_count=938449, n_uniq=3234\n",
      "Column V333: float64 -> float32, na_count=938449, n_uniq=2492\n",
      "Column V334: float64 -> float32, na_count=938449, n_uniq=221\n",
      "Column V335: float64 -> float32, na_count=938449, n_uniq=1022\n",
      "Column V336: float64 -> float32, na_count=938449, n_uniq=505\n",
      "Column V337: float64 -> float32, na_count=938449, n_uniq=438\n",
      "Column V338: float64 -> float32, na_count=938449, n_uniq=795\n",
      "Column V339: float64 -> float32, na_count=938449, n_uniq=625\n",
      "Column V34: float64 -> float16, na_count=88662, n_uniq=24\n",
      "Column V35: float64 -> float16, na_count=245823, n_uniq=6\n",
      "Column V36: float64 -> float16, na_count=245823, n_uniq=8\n",
      "Column V37: float64 -> float16, na_count=245823, n_uniq=56\n",
      "Column V38: float64 -> float16, na_count=245823, n_uniq=56\n",
      "Column V39: float64 -> float16, na_count=245823, n_uniq=23\n",
      "Column V4: float64 -> float16, na_count=455805, n_uniq=11\n",
      "Column V40: float64 -> float16, na_count=245823, n_uniq=26\n",
      "Column V41: float64 -> float16, na_count=245823, n_uniq=3\n",
      "Column V42: float64 -> float16, na_count=245823, n_uniq=10\n",
      "Column V43: float64 -> float16, na_count=245823, n_uniq=13\n",
      "Column V44: float64 -> float16, na_count=245823, n_uniq=50\n",
      "Column V45: float64 -> float16, na_count=245823, n_uniq=71\n",
      "Column V46: float64 -> float16, na_count=245823, n_uniq=10\n",
      "Column V47: float64 -> float16, na_count=245823, n_uniq=14\n",
      "Column V48: float64 -> float16, na_count=245823, n_uniq=7\n",
      "Column V49: float64 -> float16, na_count=245823, n_uniq=9\n",
      "Column V5: float64 -> float16, na_count=455805, n_uniq=12\n",
      "Column V50: float64 -> float16, na_count=245823, n_uniq=9\n",
      "Column V51: float64 -> float16, na_count=245823, n_uniq=10\n",
      "Column V52: float64 -> float16, na_count=245823, n_uniq=13\n",
      "Column V53: float64 -> float16, na_count=89995, n_uniq=7\n",
      "Column V54: float64 -> float16, na_count=89995, n_uniq=9\n",
      "Column V55: float64 -> float16, na_count=89995, n_uniq=51\n",
      "Column V56: float64 -> float16, na_count=89995, n_uniq=53\n",
      "Column V57: float64 -> float16, na_count=89995, n_uniq=8\n",
      "Column V58: float64 -> float16, na_count=89995, n_uniq=12\n",
      "Column V59: float64 -> float16, na_count=89995, n_uniq=18\n",
      "Column V6: float64 -> float16, na_count=455805, n_uniq=15\n",
      "Column V60: float64 -> float16, na_count=89995, n_uniq=19\n",
      "Column V61: float64 -> float16, na_count=89995, n_uniq=8\n",
      "Column V62: float64 -> float16, na_count=89995, n_uniq=12\n",
      "Column V63: float64 -> float16, na_count=89995, n_uniq=10\n",
      "Column V64: float64 -> float16, na_count=89995, n_uniq=12\n",
      "Column V65: float64 -> float16, na_count=89995, n_uniq=3\n",
      "Column V66: float64 -> float16, na_count=89995, n_uniq=10\n",
      "Column V67: float64 -> float16, na_count=89995, n_uniq=12\n",
      "Column V68: float64 -> float16, na_count=89995, n_uniq=9\n",
      "Column V69: float64 -> float16, na_count=89995, n_uniq=7\n",
      "Column V7: float64 -> float16, na_count=455805, n_uniq=15\n",
      "Column V70: float64 -> float16, na_count=89995, n_uniq=10\n",
      "Column V71: float64 -> float16, na_count=89995, n_uniq=8\n",
      "Column V72: float64 -> float16, na_count=89995, n_uniq=12\n",
      "Column V73: float64 -> float16, na_count=89995, n_uniq=10\n",
      "Column V74: float64 -> float16, na_count=89995, n_uniq=12\n",
      "Column V75: float64 -> float16, na_count=101245, n_uniq=7\n",
      "Column V76: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V77: float64 -> float16, na_count=101245, n_uniq=82\n",
      "Column V78: float64 -> float16, na_count=101245, n_uniq=82\n",
      "Column V79: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V8: float64 -> float16, na_count=455805, n_uniq=13\n",
      "Column V80: float64 -> float16, na_count=101245, n_uniq=21\n",
      "Column V81: float64 -> float16, na_count=101245, n_uniq=21\n",
      "Column V82: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V83: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V84: float64 -> float16, na_count=101245, n_uniq=12\n",
      "Column V85: float64 -> float16, na_count=101245, n_uniq=12\n",
      "Column V86: float64 -> float16, na_count=101245, n_uniq=82\n",
      "Column V87: float64 -> float16, na_count=101245, n_uniq=82\n",
      "Column V88: float64 -> float16, na_count=101245, n_uniq=3\n",
      "Column V89: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V9: float64 -> float16, na_count=455805, n_uniq=13\n",
      "Column V90: float64 -> float16, na_count=101245, n_uniq=7\n",
      "Column V91: float64 -> float16, na_count=101245, n_uniq=10\n",
      "Column V92: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V93: float64 -> float16, na_count=101245, n_uniq=9\n",
      "Column V94: float64 -> float16, na_count=101245, n_uniq=4\n",
      "Column V95: float64 -> float16, na_count=314, n_uniq=882\n",
      "Column V96: float64 -> float16, na_count=314, n_uniq=1411\n",
      "Column V97: float64 -> float16, na_count=314, n_uniq=977\n",
      "Column V98: float64 -> float16, na_count=314, n_uniq=14\n",
      "Column V99: float64 -> float16, na_count=314, n_uniq=90\n",
      "Column addr1: float64 -> float16, na_count=131315, n_uniq=442\n",
      "Column addr2: float64 -> float16, na_count=131315, n_uniq=94\n",
      "Column card1: int64 -> int16, na_count=0, n_uniq=17091\n",
      "Column card2: float64 -> float16, na_count=17587, n_uniq=502\n",
      "Column card3: float64 -> float16, na_count=4567, n_uniq=134\n",
      "Column card4: object -> object, na_count=4663, n_uniq=5\n",
      "Column card5: float64 -> float16, na_count=8806, n_uniq=139\n",
      "Column card6: object -> object, na_count=4578, n_uniq=5\n",
      "Column dist1: float64 -> float32, na_count=643488, n_uniq=2739\n",
      "Column dist2: float64 -> float32, na_count=1023168, n_uniq=2349\n",
      "Column id_01: float64 -> float16, na_count=811091, n_uniq=90\n",
      "Column id_02: float64 -> float32, na_count=819383, n_uniq=198052\n",
      "Column id_03: float64 -> float16, na_count=964426, n_uniq=26\n",
      "Column id_04: float64 -> float16, na_count=964426, n_uniq=17\n",
      "Column id_05: float64 -> float16, na_count=825616, n_uniq=98\n",
      "Column id_06: float64 -> float16, na_count=825616, n_uniq=102\n",
      "Column id_07: float64 -> float16, na_count=1087017, n_uniq=93\n",
      "Column id_08: float64 -> float16, na_count=1087017, n_uniq=98\n",
      "Column id_09: float64 -> float16, na_count=947967, n_uniq=52\n",
      "Column id_10: float64 -> float16, na_count=947967, n_uniq=68\n",
      "Column id_11: float64 -> float32, na_count=819475, n_uniq=413\n",
      "Column id_12: object -> object, na_count=811091, n_uniq=3\n",
      "Column id_13: float64 -> float16, na_count=839625, n_uniq=56\n",
      "Column id_14: float64 -> float16, na_count=945830, n_uniq=29\n",
      "Column id_15: object -> object, na_count=819269, n_uniq=4\n",
      "Column id_16: object -> object, na_count=842144, n_uniq=3\n",
      "Column id_17: float64 -> float16, na_count=821896, n_uniq=128\n",
      "Column id_18: float64 -> float16, na_count=1001243, n_uniq=20\n",
      "Column id_19: float64 -> float16, na_count=822007, n_uniq=569\n",
      "Column id_20: float64 -> float16, na_count=822337, n_uniq=548\n",
      "Column id_21: float64 -> float16, na_count=1087013, n_uniq=735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column id_22: float64 -> float16, na_count=1087000, n_uniq=36\n",
      "Column id_23: object -> object, na_count=1087000, n_uniq=4\n",
      "Column id_24: float64 -> float16, na_count=1087744, n_uniq=18\n",
      "Column id_25: float64 -> float16, na_count=1087060, n_uniq=441\n",
      "Column id_26: float64 -> float16, na_count=1087021, n_uniq=116\n",
      "Column id_27: object -> object, na_count=1087000, n_uniq=3\n",
      "Column id_28: object -> object, na_count=819475, n_uniq=3\n",
      "Column id_29: object -> object, na_count=819475, n_uniq=3\n",
      "Column id_30: object -> object, na_count=949007, n_uniq=88\n",
      "Column id_31: object -> object, na_count=820324, n_uniq=173\n",
      "Column id_32: float64 -> float16, na_count=948974, n_uniq=7\n",
      "Column id_33: object -> object, na_count=953271, n_uniq=462\n",
      "Column id_34: object -> object, na_count=947251, n_uniq=5\n",
      "Column id_35: object -> object, na_count=819269, n_uniq=3\n",
      "Column id_36: object -> object, na_count=819269, n_uniq=3\n",
      "Column id_37: object -> object, na_count=819269, n_uniq=3\n",
      "Column id_38: object -> object, na_count=819269, n_uniq=3\n",
      "Column isFraud: int64 -> int8, na_count=0, n_uniq=2\n",
      "Mem. usage decreased from 4819.73 Mb to 2465.33 Mb (48.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_test = reduce_mem_usage_sd(train_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 434), (506691, 433))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_test[:590540]\n",
    "test = train_test[590540:].drop('isFraud', axis=1)\n",
    "\n",
    "del train_test\n",
    "gc.collect()\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Cards Nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the whole idea behind the function count_unqies is to find out for each value card1 how many values does card2,card3,card4, card5 and card5 have in common. For example run the folowing the code seperately and \n",
    "\n",
    "unique_train = []\n",
    "unique_test = []\n",
    "for value in train['card1'].unique():\n",
    "    unique_train.append(train['card2'][train['card1'] == value].value_counts().shape[0])\n",
    "    \n",
    "for value in test['card1'].unique():\n",
    "    unique_test.append(test['card2'][test['card1'] == value].value_counts().shape[0])\n",
    "    \n",
    "pair_values_train = pd.Series(data=unique_train, index=train['card1'].unique())\n",
    "pair_values_test = pd.Series(data=unique_test, index=test['card1'].unique())\n",
    "pd.concat([pair_values_train.value_counts(), pair_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})\n",
    "\n",
    "pair_values_train[pair_values_train.values == 2]\n",
    "train['card2'][train['card1'] == 2581].value_counts()\n",
    "\n",
    "We could see the two distinct values for the value of card1 = 2581 in card2.\n",
    "\n",
    "https://www.kaggle.com/grazder/filling-card-nans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uniques(train, test, pair):\n",
    "    unique_train = []\n",
    "    unique_test = []\n",
    "\n",
    "    for value in train[pair[0]].unique():\n",
    "        unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n",
    "\n",
    "    for value in test[pair[0]].unique():\n",
    "        unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n",
    "\n",
    "    pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n",
    "    pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n",
    "    \n",
    "    return pair_values_train, pair_values_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fill_card_nans get the pair_values_train and pair_values_test from the count_uniques and then it fills all the values of card2,card3..card6 with each value that corresponds in card1. Note card1 values that have only a single occurence are taken here I guess. Becuase the for loop is taking into account values that have pair_values_train == 1. I am not sure about the occurences for if a card1 value has say 2-3 distinct values for card2,card3...card6\n",
    "\n",
    "However we can see below that in the first part of the function we can see the amount isNan's in training and testing data in the range of 8933 and 8654. However it has been bought down to 4780, 5511, this corresponds to the idea of those unique values in the case of 2 occurences and 3 occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_card_nans(train, test, pair_values_train, pair_values_test, pair):\n",
    "    print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n",
    "    print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n",
    "\n",
    "    print('Filling train...')\n",
    "    \n",
    "    for value in pair_values_train[pair_values_train == 1].index:\n",
    "        train[pair[1]][train[pair[0]] == value] = train[pair[1]][train[pair[0]] == value].value_counts().index[0]\n",
    "        \n",
    "    print('Filling test...')\n",
    "\n",
    "    for value in pair_values_test[pair_values_test == 1].index:\n",
    "        test[pair[1]][test[pair[0]] == value] = test[pair[1]][test[pair[0]] == value].value_counts().index[0]\n",
    "        \n",
    "    print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n",
    "    print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n",
    "    \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function nans_distribtution gives the number of nan's values for each card2,card3...card6 returns the distrbution \n",
    "of the number of Nan's in 0,1 & 2 categoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nans_distribution(train, test, unique_train, unique_test, pair):\n",
    "    train_nans_per_category = []\n",
    "    test_nans_per_category = []\n",
    "\n",
    "    for value in unique_train.unique():\n",
    "        train_nans_per_category.append(train[train[pair[0]].isin(list(unique_train[unique_train == value].index))][pair[1]].isna().sum())\n",
    "\n",
    "    for value in unique_test.unique():\n",
    "        test_nans_per_category.append(test[test[pair[0]].isin(list(unique_test[unique_test == value].index))][pair[1]].isna().sum())\n",
    "\n",
    "    pair_values_train = pd.Series(data=train_nans_per_category, index=unique_train.unique())\n",
    "    pair_values_test = pd.Series(data=test_nans_per_category, index=unique_test.unique())\n",
    "    \n",
    "    return pair_values_train, pair_values_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train['card2'] there are 8933 NaNs\n",
      "In test['card2'] there are 8654 NaNs\n",
      "Filling train...\n",
      "Filling test...\n",
      "In train['card2'] there are 4780 NaNs\n",
      "In test['card2'] there are 5511 NaNs\n",
      "In train['card3'] there are 1565 NaNs\n",
      "In test['card3'] there are 3002 NaNs\n",
      "Filling train...\n",
      "Filling test...\n",
      "In train['card3'] there are 17 NaNs\n",
      "In test['card3'] there are 48 NaNs\n",
      "In train['card4'] there are 1577 NaNs\n",
      "In test['card4'] there are 3086 NaNs\n",
      "Filling train...\n",
      "Filling test...\n",
      "In train['card4'] there are 27 NaNs\n",
      "In test['card4'] there are 130 NaNs\n",
      "In train['card5'] there are 4259 NaNs\n",
      "In test['card5'] there are 4547 NaNs\n",
      "Filling train...\n",
      "Filling test...\n",
      "In train['card5'] there are 939 NaNs\n",
      "In test['card5'] there are 1449 NaNs\n",
      "In train['card6'] there are 1571 NaNs\n",
      "In test['card6'] there are 3007 NaNs\n",
      "Filling train...\n",
      "Filling test...\n",
      "In train['card6'] there are 26 NaNs\n",
      "In test['card6'] there are 54 NaNs\n",
      "Wall time: 53min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for card in ['card2','card3','card4','card5','card6']:\n",
    "    unique_values_train, unique_values_test = count_uniques(train, test, ('card1', card))\n",
    "    train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', card))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>327.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5   card6  addr1  addr2  dist1  dist2  \\\n",
       "0  327.0  150.0    discover  142.0  credit  315.0   87.0   19.0    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  credit  325.0   87.0    NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0   debit  330.0   87.0  287.0    NaN   \n",
       "3  567.0  150.0  mastercard  117.0   debit  476.0   87.0    NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  credit  420.0   87.0    NaN    NaN   \n",
       "\n",
       "  P_emaildomain R_emaildomain     D1     D2    D3    D4   D5  D6  D7  D8  D9  \\\n",
       "0           NaN           NaN   14.0    NaN  13.0   NaN  NaN NaN NaN NaN NaN   \n",
       "1     gmail.com           NaN    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   \n",
       "2   outlook.com           NaN    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   \n",
       "3     yahoo.com           NaN  112.0  112.0   0.0  94.0  0.0 NaN NaN NaN NaN   \n",
       "4     gmail.com           NaN    0.0    NaN   NaN   NaN  NaN NaN NaN NaN NaN   \n",
       "\n",
       "    D10    D11  D12  D13  D14    D15  C1  C2  C3  C4  C5  C6  C7  C8  C9  C10  \\\n",
       "0  13.0   13.0  NaN  NaN  NaN    0.0   1   1   0   0   0   1   0   0   1    0   \n",
       "1   0.0    NaN  NaN  NaN  NaN    0.0   1   1   0   0   0   1   0   0   0    0   \n",
       "2   0.0  315.0  NaN  NaN  NaN  315.0   1   1   0   0   0   1   0   0   1    0   \n",
       "3  84.0    NaN  NaN  NaN  NaN  111.0   2   5   0   0   0   4   0   0   1    0   \n",
       "4   NaN    NaN  NaN  NaN  NaN    NaN   1   1   0   0   0   1   0   1   0    1   \n",
       "\n",
       "   C11  C12  C13  C14  \n",
       "0    2    0    1    1  \n",
       "1    1    0    1    1  \n",
       "2    1    0    1    1  \n",
       "3    1    0   25    1  \n",
       "4    1    0    1    1  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D1 column holds the cumulative number of days since the day the transanction started. Therefore subtracting that from the starting day gives the number of days since the card has been operational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>DaysFromStart</th>\n",
       "      <th>D1-DaysFromStart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>327.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5   card6  addr1  addr2  dist1  dist2  \\\n",
       "0  327.0  150.0    discover  142.0  credit  315.0   87.0   19.0    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  credit  325.0   87.0    NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0   debit  330.0   87.0  287.0    NaN   \n",
       "3  567.0  150.0  mastercard  117.0   debit  476.0   87.0    NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  credit  420.0   87.0    NaN    NaN   \n",
       "\n",
       "  P_emaildomain R_emaildomain     D1     D2    D3    D4   D5  D6  D7  D8  D9  \\\n",
       "0           NaN           NaN   14.0    NaN  13.0   NaN  NaN NaN NaN NaN NaN   \n",
       "1     gmail.com           NaN    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   \n",
       "2   outlook.com           NaN    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   \n",
       "3     yahoo.com           NaN  112.0  112.0   0.0  94.0  0.0 NaN NaN NaN NaN   \n",
       "4     gmail.com           NaN    0.0    NaN   NaN   NaN  NaN NaN NaN NaN NaN   \n",
       "\n",
       "    D10    D11  D12  D13  D14    D15  C1  C2  C3  C4  C5  C6  C7  C8  C9  C10  \\\n",
       "0  13.0   13.0  NaN  NaN  NaN    0.0   1   1   0   0   0   1   0   0   1    0   \n",
       "1   0.0    NaN  NaN  NaN  NaN    0.0   1   1   0   0   0   1   0   0   0    0   \n",
       "2   0.0  315.0  NaN  NaN  NaN  315.0   1   1   0   0   0   1   0   0   1    0   \n",
       "3  84.0    NaN  NaN  NaN  NaN  111.0   2   5   0   0   0   4   0   0   1    0   \n",
       "4   NaN    NaN  NaN  NaN  NaN    NaN   1   1   0   0   0   1   0   1   0    1   \n",
       "\n",
       "   C11  C12  C13  C14  DaysFromStart  D1-DaysFromStart  \n",
       "0    2    0    1    1            0.0              14.0  \n",
       "1    1    0    1    1            0.0               0.0  \n",
       "2    1    0    1    1            0.0               0.0  \n",
       "3    1    0   25    1            0.0             112.0  \n",
       "4    1    0    1    1            0.0               0.0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in [train,test]:\n",
    "    df['DaysFromStart'] = np.floor(df['TransactionDT']/(60*60*24)) - 1\n",
    "    df['D1-DaysFromStart'] = df['D1'] - df['DaysFromStart']\n",
    "cols = cols + ['DaysFromStart','D1-DaysFromStart']\n",
    "train[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next finding the Unique Idnetification is the distinguishing step that made the difference in the competition. It was found  that certain columns clubbed together could help indentify unique transaction ID's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>DaysFromStart</th>\n",
       "      <th>D1-DaysFromStart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>W_13926_327.0_150.0_discover_142.0_credit_315.0_14.0</td>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>327.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>W_2755_404.0_150.0_mastercard_102.0_credit_325.0_0.0</td>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>W_4663_490.0_150.0_visa_166.0_debit_330.0_0.0</td>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>W_18132_567.0_150.0_mastercard_117.0_debit_476.0_112.0</td>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>H_4497_514.0_150.0_mastercard_102.0_credit_420.0_0.0</td>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      uid  TransactionID  \\\n",
       "0    W_13926_327.0_150.0_discover_142.0_credit_315.0_14.0        2987000   \n",
       "1    W_2755_404.0_150.0_mastercard_102.0_credit_325.0_0.0        2987001   \n",
       "2           W_4663_490.0_150.0_visa_166.0_debit_330.0_0.0        2987002   \n",
       "3  W_18132_567.0_150.0_mastercard_117.0_debit_476.0_112.0        2987003   \n",
       "4    H_4497_514.0_150.0_mastercard_102.0_credit_420.0_0.0        2987004   \n",
       "\n",
       "   isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  card3  \\\n",
       "0        0          86400            68.5         W  13926  327.0  150.0   \n",
       "1        0          86401            29.0         W   2755  404.0  150.0   \n",
       "2        0          86469            59.0         W   4663  490.0  150.0   \n",
       "3        0          86499            50.0         W  18132  567.0  150.0   \n",
       "4        0          86506            50.0         H   4497  514.0  150.0   \n",
       "\n",
       "        card4  card5   card6  addr1  addr2  dist1  dist2 P_emaildomain  \\\n",
       "0    discover  142.0  credit  315.0   87.0   19.0    NaN           NaN   \n",
       "1  mastercard  102.0  credit  325.0   87.0    NaN    NaN     gmail.com   \n",
       "2        visa  166.0   debit  330.0   87.0  287.0    NaN   outlook.com   \n",
       "3  mastercard  117.0   debit  476.0   87.0    NaN    NaN     yahoo.com   \n",
       "4  mastercard  102.0  credit  420.0   87.0    NaN    NaN     gmail.com   \n",
       "\n",
       "  R_emaildomain     D1     D2    D3    D4   D5  D6  D7  D8  D9   D10    D11  \\\n",
       "0           NaN   14.0    NaN  13.0   NaN  NaN NaN NaN NaN NaN  13.0   13.0   \n",
       "1           NaN    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   0.0    NaN   \n",
       "2           NaN    0.0    NaN   NaN   0.0  NaN NaN NaN NaN NaN   0.0  315.0   \n",
       "3           NaN  112.0  112.0   0.0  94.0  0.0 NaN NaN NaN NaN  84.0    NaN   \n",
       "4           NaN    0.0    NaN   NaN   NaN  NaN NaN NaN NaN NaN   NaN    NaN   \n",
       "\n",
       "   D12  D13  D14    D15  C1  C2  C3  C4  C5  C6  C7  C8  C9  C10  C11  C12  \\\n",
       "0  NaN  NaN  NaN    0.0   1   1   0   0   0   1   0   0   1    0    2    0   \n",
       "1  NaN  NaN  NaN    0.0   1   1   0   0   0   1   0   0   0    0    1    0   \n",
       "2  NaN  NaN  NaN  315.0   1   1   0   0   0   1   0   0   1    0    1    0   \n",
       "3  NaN  NaN  NaN  111.0   2   5   0   0   0   4   0   0   1    0    1    0   \n",
       "4  NaN  NaN  NaN    NaN   1   1   0   0   0   1   0   1   0    1    1    0   \n",
       "\n",
       "   C13  C14  DaysFromStart  D1-DaysFromStart  \n",
       "0    1    1            0.0              14.0  \n",
       "1    1    1            0.0               0.0  \n",
       "2    1    1            0.0               0.0  \n",
       "3   25    1            0.0             112.0  \n",
       "4    1    1            0.0               0.0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in [train,test]:\n",
    "    df['uid'] = df['ProductCD'].astype(str) + '_' + df['card1'].astype(str) + '_' + df['card2'].astype(str)\n",
    "    df['uid'] = df['uid'] + '_' + df['card3'].astype(str) + '_' + df['card4'].astype(str)\n",
    "    df['uid'] = df['uid'] + '_' + df['card5'].astype(str) + '_' + df['card6'].astype(str)\n",
    "    df['uid'] = df['uid'] + '_' + df['addr1'].astype(str) + '_' + df['D1-DaysFromStart'].astype(str)\n",
    "cols = ['uid'] + cols\n",
    "train[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>C_10003_555.0_128.0_visa_226.0_debit_nan_-89.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>C_1000_555.0_185.0_mastercard_224.0_debit_nan_-65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>C_10022_555.0_117.0_mastercard_224.0_debit_nan_-34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>C_10023_111.0_150.0_visa_226.0_debit_nan_-102.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>C_10023_111.0_150.0_visa_226.0_debit_nan_-114.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>C_10023_111.0_150.0_visa_226.0_debit_nan_-145.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>C_10023_111.0_150.0_visa_226.0_debit_nan_-176.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>C_10023_111.0_150.0_visa_226.0_debit_nan_-72.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>C_10024_321.0_150.0_visa_144.0_credit_nan_-136.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>C_10024_321.0_150.0_visa_144.0_credit_nan_-160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    uid  count  sum\n",
       "0        C_10003_555.0_128.0_visa_226.0_debit_nan_-89.0      5    0\n",
       "1   C_1000_555.0_185.0_mastercard_224.0_debit_nan_-65.0      1    0\n",
       "2  C_10022_555.0_117.0_mastercard_224.0_debit_nan_-34.0      1    0\n",
       "3       C_10023_111.0_150.0_visa_226.0_debit_nan_-102.0      1    0\n",
       "4       C_10023_111.0_150.0_visa_226.0_debit_nan_-114.0      1    0\n",
       "5       C_10023_111.0_150.0_visa_226.0_debit_nan_-145.0      2    0\n",
       "6       C_10023_111.0_150.0_visa_226.0_debit_nan_-176.0      1    0\n",
       "7        C_10023_111.0_150.0_visa_226.0_debit_nan_-72.0      1    0\n",
       "8      C_10024_321.0_150.0_visa_144.0_credit_nan_-136.0      3    3\n",
       "9      C_10024_321.0_150.0_visa_144.0_credit_nan_-160.0      1    0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by = ['uid']\n",
    "grouped = train.groupby(by, as_index=False)['isFraud'].agg(['count','sum']).reset_index()\n",
    "grouped.sort_values(by).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table we can see that the UID number 8 has 3 counts and 3 sum. The sum of 3 shows that all the transactions from that account has been fradulent. This is a pretty amazing find. I am pretty sure this is a good way to indentify rows that have been fraudulent. The whole idea behind the competition is to find fradulent credit cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>231339.000000</td>\n",
       "      <td>231339.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2.552704</td>\n",
       "      <td>0.089319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>5.207512</td>\n",
       "      <td>0.906427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1414.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count            sum\n",
       "count  231339.000000  231339.000000\n",
       "mean        2.552704       0.089319\n",
       "std         5.207512       0.906427\n",
       "min         1.000000       0.000000\n",
       "25%         1.000000       0.000000\n",
       "50%         1.000000       0.000000\n",
       "75%         2.000000       0.000000\n",
       "max      1414.000000      90.000000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['DaysFromStart','D1-DaysFromStart'], axis=1)\n",
    "test = test.drop(['DaysFromStart','D1-DaysFromStart'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train.to_pickle('I:/ML Datasets/IEEE Fradulent/train_reduced.pkl')\n",
    "test.to_pickle('I:/ML Datasets/IEEE Fradulent/test_reduced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_pickle('I:/ML Datasets/IEEE Fradulent/train_reduced.pkl')\n",
    "test = pd.read_pickle('I:/ML Datasets/IEEE Fradulent/test_reduced.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphviz\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few plotting defaults\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle('I:/ML Datasets/IEEE Fradulent/train_reduced.pkl')\n",
    "X_test = pd.read_pickle('I:/ML Datasets/IEEE Fradulent/test_reduced.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEED FUNCTION\n",
    "The SEED function that defines the inital input seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 ## This is the defining SEED\n",
    "seed_everything(SEED)\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "TARGET = 'isFraud'\n",
    "NFOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (590540, 434), X_test.shape : (506691, 434)\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('I:/ML Datasets/IEEE Fradulent/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "y = X[TARGET]\n",
    "X = X.drop(TARGET, axis=1)\n",
    "        \n",
    "print(f'X.shape : {X.shape}, X_test.shape : {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split_by_time(X, y, test_size=0.2):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    print(f'train.shape: {X_train.shape}, val.shape: {X_val.shape}')\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (472432, 434), val.shape: (118108, 434)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split_by_time(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(X, model):\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_df['feature'] = X.columns.tolist()\n",
    "    feature_importance_df['gain_importance'] = model.feature_importance('gain')\n",
    "    feature_importance_df['split_importance'] = model.feature_importance('split')\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(121)\n",
    "    plot_1 = sns.barplot(x='gain_importance', y='feature',\n",
    "                         data=feature_importance_df.sort_values(by='gain_importance', ascending=False)[:50])\n",
    "    plot_1.set_title('LightGBM Feature Gain Importance')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_2 = sns.barplot(x='split_importance', y='feature',\n",
    "                         data=feature_importance_df.sort_values(by='split_importance', ascending=False)[:50])\n",
    "    plot_2.set_title('LightGBM Feature Split Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(fi_df):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(121)\n",
    "    plot_1 = sns.barplot(x='gain_importance', y='feature',\n",
    "                         data=fi_df.sort_values(by='gain_importance', ascending=False)[:50])\n",
    "    plot_1.set_title('LightGBM Feature Gain Importance')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_2 = sns.barplot(x='split_importance', y='feature',\n",
    "                         data=fi_df.sort_values(by='split_importance', ascending=False)[:50])\n",
    "    plot_2.set_title('LightGBM Feature Split Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGB Parameters Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary', ## Target vlaue is binary\n",
    "    #'metric': 'auc',\n",
    "    'metric': 'None',  # Default Metric: ‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier\n",
    "    'learning_rate': 0.01, ## Learning rate\n",
    "    'num_leaves': 2**8, ## number of leaves\n",
    "    'max_bin': 255, ## This is the binning value per feature. It decides how many max bins to be created\n",
    "    'max_depth': -1, ## This means no leaves\n",
    "    'bagging_freq': 5, ## I am guessing bag at every 5th iteration\n",
    "    'bagging_fraction': 0.7,  ## 70% of parameters randomly in each iteration for building trees.\n",
    "    'bagging_seed': SEED, # Setting the seed\n",
    "    'feature_fraction': 0.7, # Amount of features to take\n",
    "    'feature_fraction_seed': SEED,\n",
    "    'first_metric_only': True, \n",
    "    #Create a callback that activates early stopping.Activates early stopping. The model will train until \n",
    "    #the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds\n",
    "    #round(s) to continue training.Requires at least one validation data and one metric. \n",
    "    #If there’s more than one, will check all of them. But the training data is ignored anyway.\n",
    "    #To check only the first metric set first_metric_only to True.\n",
    "    'verbose': 100,\n",
    "    'n_jobs': -1,\n",
    "    'seed': SEED,\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba \n",
    "\n",
    "Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN. You don't need to replace the Python interpreter, run a separate compilation step, or even have a C/C++ compiler installed. Just apply one of the Numba decorators to your Python function, and Numba does the rest.\n",
    "\n",
    "jit is one of those decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast AUC\n",
    "\n",
    "To understand the logic behind scikit's implementation of AUC-ROC curve the link is below. Note: We have to remember that \n",
    "that we are predicting the probabilites of getting a positive value and that is thhe basis of the plotting the curve.\n",
    "Therefore all the probabilities are for the prediction that whether the class belongs to 1 or not. The threshold is \n",
    "randomly produced by the algorith I guess. pos means which of these are we assinging true positive.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "\n",
    "https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "\n",
    "The link to the fast_auc source code is given above. The explanation is not clear will be adding that when I get one\n",
    "\n",
    "the fast_auc function is defnitely faster than the sci-kit implementation but however it hits a roadblock when say there\n",
    "are two values 0.5,0.5 for the same classification of 0,1. I am not fully sure behind the logic behind the code. It seems\n",
    "as if a higher weight is given for vlaues that are correctly classified at a higher threshold and then all the positive \n",
    "values are added. I dont understand the logic behind the  division "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "        print(auc)\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "def eval_auc(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'auc', fast_auc(labels, preds), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariate Shift\n",
    "\n",
    "The whole idea behind doing covariate shift is to understand whether one variable considerablly changes between the training and the testing data. There is a shift in the distribution of the data between the training and the testing. This can be indentified by mixing the testing and the training data. Create a seperate feature that, in our case we created isTest = 0 for all the training variables and isTest = 1 for all the testing dataset. This is more of part of an EDA to understand. We use ROCAUC score and assess the only feature that we are concerned about. If the score is in the range of 0.5 it means the varialbe doesnt do a good job in distinguishing between the training and testing dataset and therefore doesnt exhibhit covariate shift.\n",
    "More on this can be found at \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/\n",
    "\n",
    "The code was taken from this thread\n",
    "\n",
    "https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariate_shift(df_train, df_test, feature, seed=SEED):\n",
    "    df_f_train = pd.DataFrame(data={feature: df_train[feature], 'isTest': 0})\n",
    "    df_f_test = pd.DataFrame(data={feature: df_test[feature], 'isTest': 1})\n",
    "\n",
    "    # Creating a single dataframe\n",
    "    df = pd.concat([df_f_train, df_f_test], ignore_index=True)\n",
    "    \n",
    "    # Encoding if feature is categorical\n",
    "    if str(df[feature].dtype) in ['object', 'category']:\n",
    "        df[feature] = LabelEncoder().fit_transform(df[feature].astype(str))\n",
    "    \n",
    "    # Splitting it to a training and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[feature], df['isTest'], test_size=0.33,\n",
    "                                                        random_state=seed, stratify=df['isTest'])\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'n_estimators': 500,\n",
    "        'random_state': seed\n",
    "    }\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "    clf.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "    roc_auc =  roc_auc_score(y_test, clf.predict_proba(X_test.values.reshape(-1, 1))[:, 1])\n",
    "\n",
    "    del df, X_train, y_train, X_test, y_test\n",
    "    gc.collect();\n",
    "    \n",
    "    #print('feature:', feature, 'covariate shift:', roc_auc)\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind adversial validation is that to check each column in the train and test data together and see which columns exhibit covariate shift. The threshold is placed at 0.7 and any auc score above that means those columns have to be discarded.\n",
    "\n",
    "Also tqdm is nothing but a progress bar that displays the amount of progress made in a for loop for each iteration. It pretty much keeps us from waiting and knowing how much of progress has been made. Below is the link for that\n",
    "\n",
    "https://tqdm.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_validation(df_train, df_test, threshold=0.7):\n",
    "    list_auc_value = []\n",
    "\n",
    "    for i in tqdm(df_train.columns.tolist()):\n",
    "        auc = covariate_shift(df_test , df_train, i)\n",
    "        list_auc_value.append(auc)\n",
    "        if auc > threshold:\n",
    "            print('feature:', i, 'covariate shift:', auc)\n",
    "\n",
    "    cov = pd.Series(list_auc_value, index = df_train.columns.tolist()).sort_values() \n",
    "    list_discarded = list(cov[cov > threshold].index)\n",
    "    \n",
    "    print('features to drop:', list_discarded)\n",
    "    \n",
    "    return cov, list_discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Encoding\n",
    "\n",
    "https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68\n",
    "\n",
    "This helps improving the score on columns that are time dependant. I guess it has something to do with assiging weights to certain variables. I couldnt find a reasonable explanation that I could understand. I would have to dig in more.\n",
    "\n",
    "The timeblock_frequency_encoding function does the encoding on each time period for the combination on the train and test dataset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_encode_full(df1, df2, col, normalize=True):\n",
    "    df = pd.concat([df1[col],df2[col]])\n",
    "    vc = df.value_counts(dropna=False, normalize=normalize).to_dict()\n",
    "    nm = col + '_FE_FULL'\n",
    "    df1[nm] = df1[col].map(vc)\n",
    "    df1[nm] = df1[nm].astype('float32')\n",
    "    df2[nm] = df2[col].map(vc)\n",
    "    df2[nm] = df2[nm].astype('float32')\n",
    "    return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            print('timeblock frequency encoding:', new_col)\n",
    "            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n",
    "            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts(normalize=True).to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
    "            \n",
    "            train_df[new_col] = train_df[new_col]/train_df[period+'_FE_FULL']\n",
    "            test_df[new_col]  = test_df[new_col]/test_df[period+'_FE_FULL']\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UID Aggregation\n",
    "\n",
    "uids would be the columns.\n",
    "\n",
    "aggregations would be mean or std\n",
    "\n",
    "The main idea is to create a say take one column form uid and main column and take an aggregate such as mean. Both the dataframes (train and test) are joined togehter and it is called temp_df. A new column that merges the main columns and uid are created. The temp_df is grouped by the columns in uid and main column and then aggregated by either the mean or std and the \n",
    "aggrgeated column is called the inital new column that we created. This value is then mapped to the training and the testing \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                print('uid_aggregation:', new_col_name)\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            print('values_normalization:', new_col)\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "cols_to_drop are at the end of fe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'D5_DT_W_std_score',\n",
    "    'ProductCD_TransactionAmt_DT_W',\n",
    "    'D4_DT_D_std_score',\n",
    "    'D15_DT_D_std_score',\n",
    "    'D3_DT_W_std_score',\n",
    "    'D11_DT_W_std_score',\n",
    "    'card3_card5_DT_W_week_day_dist',\n",
    "    'card5_DT_W_week_day_dist',\n",
    "    'D10_DT_D_std_score',\n",
    "    'card3_card5_DT_D',\n",
    "    'ProductCD_cents_DT_D',\n",
    "    'D4_DT_W_std_score',\n",
    "    'D15_DT_W_std_score',\n",
    "    'uid_DT_D',\n",
    "    'card3_DT_W_week_day_dist',\n",
    "    'D10_DT_W_std_score',\n",
    "    'D8_DT_D_std_score',\n",
    "    'card3_card5_DT_W',\n",
    "    'ProductCD_cents_DT_W',\n",
    "    'uid_DT_W',\n",
    "    'D8_DT_W_std_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe1(df_train, df_test):\n",
    "    df_tr = df_train.copy()\n",
    "    df_te = df_test.copy()\n",
    "    \n",
    "    remove_features = [\n",
    "        'TransactionID','TransactionDT'\n",
    "    ]\n",
    "    \n",
    "    for df in [df_tr, df_te]:\n",
    "        # Temporary variables for aggregation\n",
    "        df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "        df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n",
    "        df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
    "        df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n",
    "\n",
    "        df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n",
    "        df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n",
    "        df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n",
    "\n",
    "        # Possible solo feature\n",
    "        df['is_december'] = df['DT'].dt.month\n",
    "        df['is_december'] = (df['is_december']==12).astype(np.int8)\n",
    "    \n",
    "    remove_features += ['DT','DT_M','DT_W','DT_D','DT_hour','DT_day_week','DT_day_month']\n",
    "    \n",
    "    for col in ['DT_W','DT_D']:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "        \n",
    "    ## We cant use it as a solo feature\n",
    "    remove_features.append(col+'_FE_FULL')\n",
    "    \n",
    "## Note the reason we cant use the time features individually is  because individually these features dont mean anything\n",
    "## what that means is that say we new for the past 7 days the normal transactions and suddenly there is a spike in activity\n",
    "## it could be due to the launch of a new iphone and therefore spike in transactions or it could be fradulent activity. \n",
    "## Without knowing what the normalcy, we dont know and cant judge. Also take a look at the test data there is a lot of missing\n",
    "## content and therefore these features cannot be used independantly used since they would vary in the train and test data.\n",
    "## Take a look at the discussion below to understand the reasoning\n",
    "    \n",
    "#https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "\n",
    "    for df in [df_tr, df_te]:\n",
    "        df['ProductCD_card1'] = df['ProductCD'].astype(str) + '_' + df['card1'].astype(str)\n",
    "        df['card1_addr1'] = df['card1'].astype(str) + '_' + df['addr1'].astype(str)\n",
    "        df['TransactionAmt_dist2'] = df['TransactionAmt'].astype(str) + '_' + df['dist2'].astype(str)\n",
    "        \n",
    "    remove_features.append('ProductCD_card1')\n",
    "    remove_features.append('card1_addr1')\n",
    "    remove_features.append('TransactionAmt_dist2')\n",
    "    \n",
    "## The next code is for removing the outliers from the code. The logic behind the loop is that if there are any unique \n",
    "## values that have a value less than 2 they would be removed. The train and test dataset are combined together before \n",
    "## doing the value counts. After which the values in the training and the testing values are copied from each and then from \n",
    "## the valid_card set. It should be noted that columns like (Product_CD, card1_addr1 ...Trans.._dist2) are created for \n",
    "## indentification of the UID.\n",
    "    \n",
    "    for col in ['card1','ProductCD_card1','card1_addr1','TransactionAmt_dist2']: \n",
    "        valid_card = pd.concat([df_tr[[col]], df_te[[col]]])\n",
    "        valid_card = valid_card[col].value_counts()\n",
    "\n",
    "        invalid_cards = valid_card[valid_card<=2]\n",
    "        print('Rare data', col, len(invalid_cards))\n",
    "\n",
    "        valid_card = valid_card[valid_card>2]\n",
    "        valid_card = list(valid_card.index)\n",
    "\n",
    "        print('No intersection in Train', col, len(df_tr[~df_tr[col].isin(df_te[col])]))\n",
    "        print('Intersection in Train', col, len(df_tr[df_tr[col].isin(df_te[col])]))\n",
    "\n",
    "        df_tr[col] = np.where(df_tr[col].isin(df_te[col]), df_tr[col], np.nan)\n",
    "        df_te[col]  = np.where(df_te[col].isin(df_tr[col]), df_te[col], np.nan)\n",
    "\n",
    "        df_tr[col] = np.where(df_tr[col].isin(valid_card), df_tr[col], np.nan)\n",
    "        df_te[col]  = np.where(df_te[col].isin(valid_card), df_te[col], np.nan)\n",
    "        print('#'*20)\n",
    "        \n",
    "## Same as the above loop but not giving a threshold rather removing values that dont correspond in each other list\n",
    "\n",
    "    for col in ['card2','card3','card4','card5','card6']: \n",
    "        print('No intersection in Train', col, len(df_tr[~df_tr[col].isin(df_te[col])]))\n",
    "        print('Intersection in Train', col, len(df_tr[df_tr[col].isin(df_te[col])]))\n",
    "\n",
    "        df_tr[col] = np.where(df_tr[col].isin(df_te[col]), df_tr[col], np.nan)\n",
    "        df_te[col]  = np.where(df_te[col].isin(df_tr[col]), df_te[col], np.nan)\n",
    "        print('#'*20)\n",
    "    \n",
    "    # Add values remove list\n",
    "    new_columns = ['uid']\n",
    "    remove_features += new_columns\n",
    "        \n",
    "    print('#'*10)\n",
    "    print('Most common uIds:')\n",
    "    for col in new_columns:\n",
    "        print('#'*10, col)\n",
    "        print(df_tr[col].value_counts()[:10])\n",
    "        \n",
    "## For all the columns below we are doing frequency encoding. This is not a time feature and therefore doing the frequency\n",
    "## encoding of it would be not an issue\n",
    "        \n",
    "    i_cols = ['card1','card2','card3','card5','ProductCD_card1','card1_addr1','TransactionAmt_dist2'] + new_columns\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "        \n",
    "        \n",
    "## The belwo columns we are grouping by certain columns\n",
    "        \n",
    "########################### card3/card5 most common hour \n",
    "# card3 or card5 is a bank country?\n",
    "# can we find:\n",
    "# - the most popular Transaction Hour\n",
    "# - the most popular Week Day\n",
    "# and then find distance from it\n",
    "\n",
    "# Prepare bank type feature\n",
    "    for df in [df_tr, df_te]:\n",
    "         df['card3_card5'] = df['card3'].astype(str) +'_'+ df['card5'].astype(str)\n",
    "    remove_features.append('card3_card5') \n",
    "    \n",
    "    encoding_mean = {\n",
    "        1: ['DT_D','DT_hour','_hour_dist','DT_hour_mean'],\n",
    "        2: ['DT_W','DT_day_week','_week_day_dist','DT_day_week_mean'],\n",
    "    }\n",
    "\n",
    "    encoding_best = {\n",
    "        1: ['DT_D','DT_hour','_hour_dist_best','DT_hour_best'],\n",
    "        2: ['DT_W','DT_day_week','_week_day_dist_best','DT_day_week_best'],\n",
    "    }\n",
    "\n",
    "## In the below code so the first part of it combines in the first case(first loop) card3 + 'DT_D' and creates a column \n",
    "## called card3_DT_D_DT_hour_mean that then groups them by this newly created column and find the mean for the hours in each\n",
    "## group. This value is then subtracted from the DT_hour.\n",
    "     # Some ugly code here (even worse than in other parts)\n",
    "    for col in ['card3','card5','card3_card5']:\n",
    "        for df in [df_tr, df_te]:\n",
    "            for encode in encoding_mean:\n",
    "                encode = encoding_mean[encode].copy()\n",
    "                new_col = col + '_' + encode[0] + encode[2]\n",
    "                df[new_col] = df[col].astype(str) +'_'+ df[encode[0]].astype(str)\n",
    "\n",
    "                temp_dict = df.groupby([new_col])[encode[1]].agg(['mean']).reset_index().rename(\n",
    "                                                                        columns={'mean': encode[3]})\n",
    "                temp_dict.index = temp_dict[new_col].values\n",
    "                temp_dict = temp_dict[encode[3]].to_dict()\n",
    "                df[new_col] = df[encode[1]] - df[new_col].map(temp_dict)\n",
    "                \n",
    "## For the second part it is similar story\n",
    "\n",
    "            for encode in encoding_best:\n",
    "                encode = encoding_best[encode].copy()\n",
    "                new_col = col + '_' + encode[0] + encode[2]\n",
    "                df[new_col] = df[col].astype(str) +'_'+ df[encode[0]].astype(str)\n",
    "                temp_dict = df.groupby([col,encode[0],encode[1]])[encode[1]].agg(['count']).reset_index().rename(\n",
    "                                                                        columns={'count': encode[3]})\n",
    "\n",
    "                temp_dict.sort_values(by=[col,encode[0],encode[3]], inplace=True)\n",
    "                temp_dict = temp_dict.drop_duplicates(subset=[col,encode[0]], keep='last')\n",
    "                temp_dict[new_col] = temp_dict[col].astype(str) +'_'+ temp_dict[encode[0]].astype(str)\n",
    "                temp_dict.index = temp_dict[new_col].values\n",
    "                temp_dict = temp_dict[encode[1]].to_dict()\n",
    "                df[new_col] = df[encode[1]] - df[new_col].map(temp_dict)\n",
    "                \n",
    "\n",
    "## The whole idea behind the block below is to check for normal activity. What that means here is take for example uid from \n",
    "## i_cols and DT_D from peroids. When they pass it onto the function. What the function does here is that it at first \n",
    "## joins both the test and train data into one single dataframe. It then creates a new column called say uid_DT_D that has\n",
    "## the uid and the dates the specific user has made transanctions. This value is then counted and normalized. What that means\n",
    "## is that say a specific customer on given date across different months has made how many transanctions. This value is then\n",
    "## divided  by the total normalized value counts of total occurences of that specific date in the total frame. Now when we\n",
    "## do that if there is a larger value this means that specific user made a large number of transactions on that specific day\n",
    "## compared to the normal transactions.\n",
    "########################### bank_type\n",
    "# Tracking nomal activity\n",
    "# by doing timeblock frequency encoding\n",
    "    i_cols = ['uid','card3_card5'] #['uid','uid2','uid3','uid4','uid5','bank_type']\n",
    "    periods = ['DT_W','DT_D']\n",
    "\n",
    "# We have few options to encode it here:\n",
    "# - Just count transactions\n",
    "#(but some timblocks have more transactions than others)\n",
    "# - Devide to total transactions per timeblock (proportions)\n",
    "# - Use both\n",
    "# - Use only proportions\n",
    "    df_tr, df_te = timeblock_frequency_encoding(df_tr, df_te, periods, i_cols)\n",
    "    \n",
    "    \n",
    "## This function is an aggregation of for example say uid,D1 and mean. We create a column called uid_D1_mean which at first\n",
    "## combines both the test and train data. This is followed by creating a temporary dataframe that groups by column uid,\n",
    "## and then does the aggregation of either mean or standard deviation over columns such as D1,D2..D16. The new dataframe \n",
    "## would be converted to a dictionary where the key would be the uid and the values would be uid_D1_mean.This would be then\n",
    "## mapped to the train and test data\n",
    "\n",
    "########################### D Columns\n",
    "    # From columns description we know that\n",
    "    # D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "    # 1. I can't imagine normal negative timedelta values (Let's clip Values)\n",
    "    # 2. Normalize (Min-Max, Standard score) All D columns, except D1,D2,D9\n",
    "    # 3. Do some aggregations based on uIDs\n",
    "    # 4. Freaquency encoding\n",
    "    # 5. D1,D2 are clipped by max train_df values (let's scale it)\n",
    "    i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "    uids = ['uid','card3_card5']\n",
    "    aggregations = ['mean','std']\n",
    "\n",
    "    ####### uIDs aggregations\n",
    "    df_tr, df_te = uid_aggregation(df_tr, df_te, i_cols, uids, aggregations)\n",
    "    \n",
    "    # Lets transform D8 and D9 column\n",
    "    # As we almost sure it has connection with hours\n",
    "    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n",
    "    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0) ## Assigning all the values greater than 1 in D8 to 1 else 0\n",
    "    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n",
    "    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n",
    "    df['D8'] = df['D8'].fillna(-1).astype(int)\n",
    "    \n",
    "    ####### Values Normalization\n",
    "    i_cols.remove('D1')\n",
    "    i_cols.remove('D2')\n",
    "    i_cols.remove('D9')\n",
    "    periods = ['DT_D','DT_W']\n",
    "    \n",
    "## Here what happens is that when say a column called DT_D for period and D3 for i_cols. It groups by the period DT_D and \n",
    "## then finds the minimum value in each group for a given date. This is then saved into a dataframe temp_min with the index\n",
    "## value being the dates and the values holding the minimum in each group. This is then done for max, mean  and std. These\n",
    "## values are mapped for the training dataset and then min_max and std_score are calcualted. All the other columns are \n",
    "## removed except for min_max and std_score.\n",
    "    \n",
    "    #kaggle.com/kyakovlev/ieee-columns-scaling\n",
    "    for df in [df_tr, df_te]:\n",
    "        df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "    for col in ['D1','D2']:\n",
    "        for df in [df_tr, df_te]:\n",
    "            df[col+'_scaled'] = df[col]/df_tr[col].max()\n",
    "\n",
    "    ####### Global Self frequency encoding\n",
    "    # self_encoding=True because \n",
    "    # we don't need original values anymore\n",
    "    i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "    remove_features += i_cols\n",
    "    \n",
    "    ####### Global Self frequency encoding\n",
    "    # self_encoding=True because \n",
    "    # we don't need original values anymore\n",
    "    i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "    remove_features += i_cols\n",
    "    \n",
    "    ########################### TransactionAmt\n",
    "    # Clip Values\n",
    "    ## clipping valuee because at a higher value the binning is easier for the lgbm models\n",
    "    df_tr['TransactionAmt'] = df_tr['TransactionAmt'].clip(0,5000)\n",
    "    df_te['TransactionAmt']  = df_te['TransactionAmt'].clip(0,5000)\n",
    "    \n",
    "    # For our model current TransactionAmt is a noise\n",
    "    # https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "    # (even if features importances are telling contrariwise)\n",
    "    # There are many unique values and model doesn't generalize well\n",
    "    # Lets do some aggregations\n",
    "    i_cols = ['TransactionAmt']\n",
    "    uids = ['card1','card2','card3','card5','uid','card3_card5']\n",
    "    aggregations = ['mean','std']\n",
    "\n",
    "    # uIDs aggregations\n",
    "    df_tr, df_te = uid_aggregation(df_tr, df_te, i_cols, uids, aggregations)\n",
    "    \n",
    "    \n",
    "    # TransactionAmt Normalization\n",
    "    periods = ['DT_D','DT_W']\n",
    "    for df in [df_tr, df_te]:\n",
    "        df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "    #i_cols = ['id_01','id_02','id_05','id_06','id_07','id_08','id_09','id_14']\n",
    "    i_cols = ['id_01','id_02','id_05','id_06','id_09','id_14']\n",
    "    uids = ['card1','card2','card3','card5','uid','card3_card5']\n",
    "    aggregations = ['mean','std']\n",
    "\n",
    "    # uIDs aggregations\n",
    "    df_tr, df_te = uid_aggregation(df_tr, df_te, i_cols, uids, aggregations)\n",
    "    \n",
    "    \n",
    "    i_cols = [\n",
    "        #'V202','V203','V204','V317','V318','V257','V258',\n",
    "        'V258',\n",
    "        'V306','V307','V308','V294'\n",
    "    ]\n",
    "    uids = ['uid','card3_card5']\n",
    "    aggregations = ['mean','std']\n",
    "    \n",
    "    # uIDs aggregations\n",
    "    df_tr, df_te = uid_aggregation(df_tr, df_te, i_cols, uids, aggregations)\n",
    "    \n",
    "    # ProductCD, TransactionAmt\n",
    "    \n",
    "## Creating new columns that are the aggregations of the ProductCD and TransactionAmt\n",
    "    df_tr['ProductCD_TransactionAmt'] = df_tr['ProductCD'].astype(str)+'_'+df_tr['TransactionAmt'].astype(str)\n",
    "    df_te['ProductCD_TransactionAmt'] = df_te['ProductCD'].astype(str)+'_'+df_te['TransactionAmt'].astype(str)\n",
    "    \n",
    "## Timeblock frequency encoding is done for ProductCD_TransactionAmt with periods for DT_D and DT_W . \n",
    "    i_cols = ['ProductCD_TransactionAmt']\n",
    "    periods = ['DT_D','DT_W']\n",
    "    df_tr, df_te = timeblock_frequency_encoding(df_tr, df_te, periods, i_cols)\n",
    "\n",
    "## Frequency encoding is done for ProductionCD_TransactionAmt   \n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "    remove_features += i_cols\n",
    "    \n",
    "## Another new column is created that has the cents which is the difference between Transaction amount and its decimal part\n",
    "\n",
    "    df_tr['cents'] = np.round( df_tr['TransactionAmt'] - np.floor(df_tr['TransactionAmt']),3 )\n",
    "    df_te['cents'] = np.round( df_te['TransactionAmt'] - np.floor(df_te['TransactionAmt']),3 )\n",
    "\n",
    "    i_cols = ['cents']\n",
    "    remove_features += i_cols\n",
    "\n",
    "## ProductCD, cents together are clubbed to create a column and then frequency encoding\n",
    "    # ProductCD, cents\n",
    "    df_tr['ProductCD_cents'] = df_tr['ProductCD'].astype(str)+'_'+df_tr['cents'].astype(str)\n",
    "    df_te['ProductCD_cents'] = df_te['ProductCD'].astype(str)+'_'+df_te['cents'].astype(str)\n",
    "\n",
    "    i_cols = ['ProductCD_cents']\n",
    "    periods = ['DT_D','DT_W']\n",
    "    df_tr, df_te = timeblock_frequency_encoding(df_tr, df_te, periods, i_cols)\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "    remove_features += i_cols\n",
    "    \n",
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "\n",
    "    df_tr['TransactionAmt'] = np.log1p(df_tr['TransactionAmt'])\n",
    "    df_te['TransactionAmt'] = np.log1p(df_te['TransactionAmt'])\n",
    "    \n",
    "########################### C Columns\n",
    "\n",
    "## So in the below code all the columns from C1 to C15 is clubbed together in one single column called c_cols_0_bin. This \n",
    "## new column is then added values from each column. This is done by checking each column and seeing if the value is 0 or 1\n",
    "## If the value is any value other than zero than it assigns 0 for that columns otherwise 1 for that columns. Therefore the\n",
    "## the output would be astrings of 0's and 1's\n",
    "\n",
    "    i_cols = ['C'+str(i) for i in range(1,15)]\n",
    "    \n",
    "    for df in [df_tr, df_te]:\n",
    "        df['c_cols_0_bin'] = ''\n",
    "        for c in i_cols:\n",
    "            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)    \n",
    "    freq_encode_full(df_tr, df_te, 'c_cols_0_bin')\n",
    "\n",
    "## In the below code fruther aggrgations are done with the sets of uid's passed onto the list and the aggregations. \n",
    "    ####### Global Self frequency encoding\n",
    "    # self_encoding=False because  ## This option is not included in the function\n",
    "    # I want to keep original values\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "\n",
    "    uids = ['card1','card2','card3','card5','uid','card3_card5']\n",
    "    aggregations = ['mean','std']\n",
    "\n",
    "    ####### uIDs aggregations\n",
    "    df_tr, df_te = uid_aggregation(df_tr, df_te, i_cols, uids, aggregations)\n",
    "    \n",
    "## For clipping the max values we find the max value of DT_M for each column C1-C15 and then take the max value at that \n",
    "## point for C1-C15 and that would be the upper clip for that specific column\n",
    "    \n",
    "    ####### Clip max values\n",
    "    for df in [df_tr, df_te]:\n",
    "        for col in i_cols:\n",
    "            max_value = df_tr[df_tr['DT_M']==df_tr['DT_M'].max()][col].max()\n",
    "            df[col] = df[col].clip(None,max_value) \n",
    "    \n",
    "       \n",
    "    ########################### dist1, dist2 Columns\n",
    "    i_cols = ['dist1']\n",
    "    uids = ['card1','card2','card3','card5','uid','card3_card5']\n",
    "    aggregations = ['mean','std']\n",
    "\n",
    "    ####### uIDs aggregations\n",
    "    df_tr, df_te = uid_aggregation(df_tr, df_te, i_cols, uids, aggregations)\n",
    "    \n",
    "## What happens here is that, atfirst a dictionary is created called nans_group and then a dataframe nans_df\n",
    "## is created with a True or False sort of a dataframe, i.e: if a given row is nan or not. Then we create a loop and \n",
    "## indentify all the columns that have Nan's greater than 0. This is then saved to the nans_group with the count of Nan as \n",
    "## the key and the column name as the count. After that another loop is done where new columns are created with the columns\n",
    "## in the dictionary and the sum of those value are added,mean and std is calculated for each key and value. the columns\n",
    "## V1-V340 are then removed.\n",
    "    \n",
    " ####### V feature - nan group agg\n",
    "    nans_groups = {}\n",
    "    nans_df = pd.concat([df_tr, df_te]).isna()\n",
    "\n",
    "    i_cols = ['V'+str(i) for i in range(1,340)]\n",
    "    for col in i_cols:\n",
    "        cur_group = nans_df[col].sum()\n",
    "        if cur_group>0:\n",
    "            try:\n",
    "                nans_groups[cur_group].append(col)\n",
    "            except:\n",
    "                nans_groups[cur_group]=[col]\n",
    "\n",
    "    for i, (n_group, n_cols) in enumerate(nans_groups.items()):\n",
    "        print('processing nan group agg for:', n_cols)\n",
    "        for df in [df_tr, df_te]:\n",
    "            df[f'nan_group_{i}_sum'] = df[n_cols].sum(axis=1)\n",
    "            df[f'nan_group_{i}_mean'] = df[n_cols].mean(axis=1)\n",
    "            df[f'nan_group_{i}_std'] = df[n_cols].std(axis=1)\n",
    "\n",
    "    del nans_groups, nans_df\n",
    "    remove_features += i_cols\n",
    "    \n",
    "       ########################### Device info and identity\n",
    "    for df in [df_tr, df_te]:\n",
    "        ########################### Device info\n",
    "        df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "        df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "        df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "        ########################### Device info 2\n",
    "        df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "        df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "        df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "        ########################### Browser\n",
    "        df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "        df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "\n",
    "    i_cols = [\n",
    "        'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "        'id_30','id_30_device','id_30_version',\n",
    "        'id_31','id_31_device',\n",
    "        'id_33',\n",
    "    ]\n",
    "    \n",
    "    ########################### Device info and identity\n",
    "    for df in [df_tr, df_te]:\n",
    "        ########################### Device info\n",
    "        df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "        df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "        df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "        ########################### Device info 2\n",
    "        df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "        df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "        df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "        ########################### Browser\n",
    "        df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "        df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "\n",
    "    i_cols = [\n",
    "        'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "        'id_30','id_30_device','id_30_version',\n",
    "        'id_31','id_31_device',\n",
    "        'id_33',\n",
    "    ]\n",
    "    \n",
    "    i_cols = [\n",
    "        'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "        'id_30','id_30_device','id_30_version',\n",
    "        'id_31','id_31_device',\n",
    "        'id_33',\n",
    "    ]\n",
    "\n",
    "    ####### Global Self frequency encoding\n",
    "    # self_encoding=True because \n",
    "    # we don't need original values anymore\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "    remove_features += i_cols\n",
    "    \n",
    "    i_cols = [\n",
    "        'id_01',\n",
    "        'id_03',\n",
    "        'id_04',\n",
    "        'id_05',\n",
    "        'id_06',\n",
    "        'id_07',\n",
    "        'id_08',\n",
    "        'id_09',\n",
    "        'id_10',\n",
    "        'id_11',\n",
    "        'id_13',\n",
    "        'id_14',\n",
    "        'id_17',\n",
    "        'id_18',\n",
    "        'id_19',\n",
    "        'id_20',\n",
    "        'id_21',\n",
    "        'id_22',\n",
    "        'id_24',\n",
    "        'id_25',\n",
    "        'id_26',\n",
    "    ]\n",
    "    for col in i_cols:\n",
    "        freq_encode_full(df_tr, df_te, col)\n",
    "        \n",
    "        \n",
    "    # Label Encoding\n",
    "    for f in df_tr.columns:\n",
    "        if df_tr[f].dtype=='object' or df_te[f].dtype=='object':\n",
    "            df_tr[f] = df_tr[f].fillna('unseen_before_label')\n",
    "            df_te[f] = df_te[f].fillna('unseen_before_label')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(df_tr[f].values) + list(df_te[f].values))\n",
    "            df_tr[f] = lbl.transform(list(df_tr[f].values))\n",
    "            df_te[f] = lbl.transform(list(df_te[f].values))\n",
    "            df_tr[f] = df_tr[f].astype('category')\n",
    "            df_te[f] = df_te[f].astype('category')\n",
    "\n",
    "    remove_features += cols_to_drop\n",
    "    print('remove_features:', remove_features)\n",
    "    print(f'train.shape : {df_tr.shape}, test.shape : {df_te.shape}')\n",
    "    \n",
    "    ########################### Final features list\n",
    "    feature_columns = [col for col in list(df_tr) if col not in remove_features]\n",
    "    categorical_features = [col for col in feature_columns if df_tr[col].dtype.name == 'category']\n",
    "    categorical_features = [col for col in categorical_features if col not in remove_features]\n",
    "    \n",
    "    return df_tr[feature_columns], df_te[feature_columns], categorical_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make_val_predictions\n",
    "\n",
    "valid_sets:  (list of Datasets or None, optional (default=None)) – List of data to be evaluated on during training\n",
    "\n",
    "Note Lightgbm uses categorical features directly without one hot encoding hence those features are being sent directly into categorical feature\n",
    "\n",
    "1000: boosting iterations\n",
    "\n",
    "\n",
    "feval = Customized evaluation function. Should accept two parameters: preds, train_data, and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
    "\n",
    "Evaulation technique:\n",
    "eval_namestring( The name of evaluation(wihtout whitespaces): eval_auc \n",
    "\n",
    "Verbose_eval:With verbose_eval = 4 and at least one item in valid_sets, an evaluation metric is printed every 4 (instead of 1) boosting stages.\n",
    "\n",
    "Best Iteration:The index of iteration that has the best performance will be saved in the best_iteration field if early stopping logic is enabled by setting early_stopping_rounds. Note that train() will return a model from the best iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_prediction(X_train, y_train, X_val, y_val, seed=SEED, seed_range=3, lgb_params=lgb_params,\n",
    "                        category_cols=None):\n",
    "    print(X_train.shape, X_val.shape)\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    auc_arr = []\n",
    "    best_iteration_arr = []\n",
    "    val_preds = np.zeros((X_val.shape[0], seed_range)) ## Array of the format (118108, 3)\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_df['feature'] = X_train.columns.tolist()\n",
    "    feature_importance_df['gain_importance'] = 0\n",
    "    feature_importance_df['split_importance'] = 0\n",
    "    \n",
    "    for i, s in enumerate(range(seed, seed + seed_range)):\n",
    "        seed_everything(s)\n",
    "        params = lgb_params.copy()  ## Creating a copy of the lgb_params defined above \n",
    "        params['seed'] = s # Assinging that seed to those parameters\n",
    "        params['bagging_seed'] = s\n",
    "        params['feature_fraction_seed'] = s\n",
    "\n",
    "        clf = lgb.train(params, train_data, 10000, valid_sets = [train_data, val_data], categorical_feature=category_cols,\n",
    "                        early_stopping_rounds=500, feval=eval_auc, verbose_eval=200)\n",
    "         \n",
    "\n",
    "        best_iteration = clf.best_iteration\n",
    "        best_iteration_arr.append(best_iteration)\n",
    "        val_pred = clf.predict(X_val, best_iteration)\n",
    "        val_preds[:, i] = val_pred\n",
    "        \n",
    "        auc = fast_auc(y_val, val_pred)\n",
    "        auc_arr.append(auc)\n",
    "        print('seed:', s, ', auc:', auc, ', best_iteration:', best_iteration)\n",
    "\n",
    "        feature_importance_df['gain_importance'] += clf.feature_importance('gain')/seed_range\n",
    "        feature_importance_df['split_importance'] += clf.feature_importance('split')/seed_range\n",
    "\n",
    "    auc_arr = np.array(auc_arr)\n",
    "    best_iteration_arr = np.array(best_iteration_arr)\n",
    "    best_iteration = int(np.mean(best_iteration_arr))\n",
    "\n",
    "    avg_pred_auc = fast_auc(y_val, np.mean(val_preds, axis=1))\n",
    "    print(f'avg pred auc: {avg_pred_auc:.5f}, avg auc: {np.mean(auc_arr):.5f}+/-{np.std(auc_arr):.5f}, avg best iteration: {best_iteration}')\n",
    "\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='split_importance', ascending=False).reset_index(drop=True)\n",
    "    plot_feature_importance(feature_importance_df)\n",
    "    display(feature_importance_df.head(20))\n",
    "    \n",
    "    return feature_importance_df, best_iteration, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare data card1 5134\n",
      "No intersection in Train card1 20399\n",
      "Intersection in Train card1 452033\n",
      "####################\n",
      "Rare data ProductCD_card1 10509\n",
      "No intersection in Train ProductCD_card1 33115\n",
      "Intersection in Train ProductCD_card1 439317\n",
      "####################\n",
      "Rare data card1_addr1 21640\n",
      "No intersection in Train card1_addr1 57867\n",
      "Intersection in Train card1_addr1 414565\n",
      "####################\n",
      "Rare data TransactionAmt_dist2 29957\n",
      "No intersection in Train TransactionAmt_dist2 70254\n",
      "Intersection in Train TransactionAmt_dist2 402178\n",
      "####################\n",
      "No intersection in Train card2 6102\n",
      "Intersection in Train card2 466330\n",
      "####################\n",
      "No intersection in Train card3 146\n",
      "Intersection in Train card3 472286\n",
      "####################\n",
      "No intersection in Train card4 0\n",
      "Intersection in Train card4 472432\n",
      "####################\n",
      "No intersection in Train card5 7339\n",
      "Intersection in Train card5 465093\n",
      "####################\n",
      "No intersection in Train card6 45\n",
      "Intersection in Train card6 472387\n",
      "####################\n",
      "##########\n",
      "Most common uIds:\n",
      "########## uid\n",
      "W_9500_321.0_150.0_visa_226.0_debit_126.0_86.0      361\n",
      "W_8900_385.0_150.0_visa_226.0_debit_231.0_61.0      232\n",
      "C_12616_490.0_150.0_visa_nan_credit_nan_492.0       202\n",
      "W_12741_106.0_150.0_visa_226.0_debit_143.0_203.0    196\n",
      "W_8528_215.0_150.0_visa_226.0_debit_387.0_160.0     188\n",
      "C_15885_545.0_185.0_visa_138.0_debit_nan_-21.0      169\n",
      "W_7207_111.0_150.0_visa_226.0_debit_204.0_466.0     168\n",
      "W_4121_361.0_150.0_visa_226.0_credit_476.0_9.0      137\n",
      "C_15885_545.0_185.0_visa_138.0_debit_nan_-26.0      137\n",
      "C_15885_545.0_185.0_visa_138.0_debit_nan_-19.0      134\n",
      "Name: uid, dtype: int64\n",
      "timeblock frequency encoding: uid_DT_W\n",
      "timeblock frequency encoding: card3_card5_DT_W\n",
      "timeblock frequency encoding: uid_DT_D\n",
      "timeblock frequency encoding: card3_card5_DT_D\n",
      "uid_aggregation: uid_D1_mean\n",
      "uid_aggregation: uid_D1_std\n",
      "uid_aggregation: card3_card5_D1_mean\n",
      "uid_aggregation: card3_card5_D1_std\n",
      "uid_aggregation: uid_D2_mean\n",
      "uid_aggregation: uid_D2_std\n",
      "uid_aggregation: card3_card5_D2_mean\n",
      "uid_aggregation: card3_card5_D2_std\n",
      "uid_aggregation: uid_D3_mean\n",
      "uid_aggregation: uid_D3_std\n",
      "uid_aggregation: card3_card5_D3_mean\n",
      "uid_aggregation: card3_card5_D3_std\n",
      "uid_aggregation: uid_D4_mean\n",
      "uid_aggregation: uid_D4_std\n",
      "uid_aggregation: card3_card5_D4_mean\n",
      "uid_aggregation: card3_card5_D4_std\n",
      "uid_aggregation: uid_D5_mean\n",
      "uid_aggregation: uid_D5_std\n",
      "uid_aggregation: card3_card5_D5_mean\n",
      "uid_aggregation: card3_card5_D5_std\n",
      "uid_aggregation: uid_D6_mean\n",
      "uid_aggregation: uid_D6_std\n",
      "uid_aggregation: card3_card5_D6_mean\n",
      "uid_aggregation: card3_card5_D6_std\n",
      "uid_aggregation: uid_D7_mean\n",
      "uid_aggregation: uid_D7_std\n",
      "uid_aggregation: card3_card5_D7_mean\n",
      "uid_aggregation: card3_card5_D7_std\n",
      "uid_aggregation: uid_D8_mean\n",
      "uid_aggregation: uid_D8_std\n",
      "uid_aggregation: card3_card5_D8_mean\n",
      "uid_aggregation: card3_card5_D8_std\n",
      "uid_aggregation: uid_D9_mean\n",
      "uid_aggregation: uid_D9_std\n",
      "uid_aggregation: card3_card5_D9_mean\n",
      "uid_aggregation: card3_card5_D9_std\n",
      "uid_aggregation: uid_D10_mean\n",
      "uid_aggregation: uid_D10_std\n",
      "uid_aggregation: card3_card5_D10_mean\n",
      "uid_aggregation: card3_card5_D10_std\n",
      "uid_aggregation: uid_D11_mean\n",
      "uid_aggregation: uid_D11_std\n",
      "uid_aggregation: card3_card5_D11_mean\n",
      "uid_aggregation: card3_card5_D11_std\n",
      "uid_aggregation: uid_D12_mean\n",
      "uid_aggregation: uid_D12_std\n",
      "uid_aggregation: card3_card5_D12_mean\n",
      "uid_aggregation: card3_card5_D12_std\n",
      "uid_aggregation: uid_D13_mean\n",
      "uid_aggregation: uid_D13_std\n",
      "uid_aggregation: card3_card5_D13_mean\n",
      "uid_aggregation: card3_card5_D13_std\n",
      "uid_aggregation: uid_D14_mean\n",
      "uid_aggregation: uid_D14_std\n",
      "uid_aggregation: card3_card5_D14_mean\n",
      "uid_aggregation: card3_card5_D14_std\n",
      "uid_aggregation: uid_D15_mean\n",
      "uid_aggregation: uid_D15_std\n",
      "uid_aggregation: card3_card5_D15_mean\n",
      "uid_aggregation: card3_card5_D15_std\n",
      "values_normalization: D3_DT_D\n",
      "values_normalization: D4_DT_D\n",
      "values_normalization: D5_DT_D\n",
      "values_normalization: D6_DT_D\n",
      "values_normalization: D7_DT_D\n",
      "values_normalization: D8_DT_D\n",
      "values_normalization: D10_DT_D\n",
      "values_normalization: D11_DT_D\n",
      "values_normalization: D12_DT_D\n",
      "values_normalization: D13_DT_D\n",
      "values_normalization: D14_DT_D\n",
      "values_normalization: D15_DT_D\n",
      "values_normalization: D3_DT_W\n",
      "values_normalization: D4_DT_W\n",
      "values_normalization: D5_DT_W\n",
      "values_normalization: D6_DT_W\n",
      "values_normalization: D7_DT_W\n",
      "values_normalization: D8_DT_W\n",
      "values_normalization: D10_DT_W\n",
      "values_normalization: D11_DT_W\n",
      "values_normalization: D12_DT_W\n",
      "values_normalization: D13_DT_W\n",
      "values_normalization: D14_DT_W\n",
      "values_normalization: D15_DT_W\n",
      "values_normalization: D3_DT_D\n",
      "values_normalization: D4_DT_D\n",
      "values_normalization: D5_DT_D\n",
      "values_normalization: D6_DT_D\n",
      "values_normalization: D7_DT_D\n",
      "values_normalization: D8_DT_D\n",
      "values_normalization: D10_DT_D\n",
      "values_normalization: D11_DT_D\n",
      "values_normalization: D12_DT_D\n",
      "values_normalization: D13_DT_D\n",
      "values_normalization: D14_DT_D\n",
      "values_normalization: D15_DT_D\n",
      "values_normalization: D3_DT_W\n",
      "values_normalization: D4_DT_W\n",
      "values_normalization: D5_DT_W\n",
      "values_normalization: D6_DT_W\n",
      "values_normalization: D7_DT_W\n",
      "values_normalization: D8_DT_W\n",
      "values_normalization: D10_DT_W\n",
      "values_normalization: D11_DT_W\n",
      "values_normalization: D12_DT_W\n",
      "values_normalization: D13_DT_W\n",
      "values_normalization: D14_DT_W\n",
      "values_normalization: D15_DT_W\n",
      "uid_aggregation: card1_TransactionAmt_mean\n",
      "uid_aggregation: card1_TransactionAmt_std\n",
      "uid_aggregation: card2_TransactionAmt_mean\n",
      "uid_aggregation: card2_TransactionAmt_std\n",
      "uid_aggregation: card3_TransactionAmt_mean\n",
      "uid_aggregation: card3_TransactionAmt_std\n",
      "uid_aggregation: card5_TransactionAmt_mean\n",
      "uid_aggregation: card5_TransactionAmt_std\n",
      "uid_aggregation: uid_TransactionAmt_mean\n",
      "uid_aggregation: uid_TransactionAmt_std\n",
      "uid_aggregation: card3_card5_TransactionAmt_mean\n",
      "uid_aggregation: card3_card5_TransactionAmt_std\n",
      "values_normalization: TransactionAmt_DT_D\n",
      "values_normalization: TransactionAmt_DT_W\n",
      "values_normalization: TransactionAmt_DT_D\n",
      "values_normalization: TransactionAmt_DT_W\n",
      "uid_aggregation: card1_id_01_mean\n",
      "uid_aggregation: card1_id_01_std\n",
      "uid_aggregation: card2_id_01_mean\n",
      "uid_aggregation: card2_id_01_std\n",
      "uid_aggregation: card3_id_01_mean\n",
      "uid_aggregation: card3_id_01_std\n",
      "uid_aggregation: card5_id_01_mean\n",
      "uid_aggregation: card5_id_01_std\n",
      "uid_aggregation: uid_id_01_mean\n",
      "uid_aggregation: uid_id_01_std\n",
      "uid_aggregation: card3_card5_id_01_mean\n",
      "uid_aggregation: card3_card5_id_01_std\n",
      "uid_aggregation: card1_id_02_mean\n",
      "uid_aggregation: card1_id_02_std\n",
      "uid_aggregation: card2_id_02_mean\n",
      "uid_aggregation: card2_id_02_std\n",
      "uid_aggregation: card3_id_02_mean\n",
      "uid_aggregation: card3_id_02_std\n",
      "uid_aggregation: card5_id_02_mean\n",
      "uid_aggregation: card5_id_02_std\n",
      "uid_aggregation: uid_id_02_mean\n",
      "uid_aggregation: uid_id_02_std\n",
      "uid_aggregation: card3_card5_id_02_mean\n",
      "uid_aggregation: card3_card5_id_02_std\n",
      "uid_aggregation: card1_id_05_mean\n",
      "uid_aggregation: card1_id_05_std\n",
      "uid_aggregation: card2_id_05_mean\n",
      "uid_aggregation: card2_id_05_std\n",
      "uid_aggregation: card3_id_05_mean\n",
      "uid_aggregation: card3_id_05_std\n",
      "uid_aggregation: card5_id_05_mean\n",
      "uid_aggregation: card5_id_05_std\n",
      "uid_aggregation: uid_id_05_mean\n",
      "uid_aggregation: uid_id_05_std\n",
      "uid_aggregation: card3_card5_id_05_mean\n",
      "uid_aggregation: card3_card5_id_05_std\n",
      "uid_aggregation: card1_id_06_mean\n",
      "uid_aggregation: card1_id_06_std\n",
      "uid_aggregation: card2_id_06_mean\n",
      "uid_aggregation: card2_id_06_std\n",
      "uid_aggregation: card3_id_06_mean\n",
      "uid_aggregation: card3_id_06_std\n",
      "uid_aggregation: card5_id_06_mean\n",
      "uid_aggregation: card5_id_06_std\n",
      "uid_aggregation: uid_id_06_mean\n",
      "uid_aggregation: uid_id_06_std\n",
      "uid_aggregation: card3_card5_id_06_mean\n",
      "uid_aggregation: card3_card5_id_06_std\n",
      "uid_aggregation: card1_id_09_mean\n",
      "uid_aggregation: card1_id_09_std\n",
      "uid_aggregation: card2_id_09_mean\n",
      "uid_aggregation: card2_id_09_std\n",
      "uid_aggregation: card3_id_09_mean\n",
      "uid_aggregation: card3_id_09_std\n",
      "uid_aggregation: card5_id_09_mean\n",
      "uid_aggregation: card5_id_09_std\n",
      "uid_aggregation: uid_id_09_mean\n",
      "uid_aggregation: uid_id_09_std\n",
      "uid_aggregation: card3_card5_id_09_mean\n",
      "uid_aggregation: card3_card5_id_09_std\n",
      "uid_aggregation: card1_id_14_mean\n",
      "uid_aggregation: card1_id_14_std\n",
      "uid_aggregation: card2_id_14_mean\n",
      "uid_aggregation: card2_id_14_std\n",
      "uid_aggregation: card3_id_14_mean\n",
      "uid_aggregation: card3_id_14_std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid_aggregation: card5_id_14_mean\n",
      "uid_aggregation: card5_id_14_std\n",
      "uid_aggregation: uid_id_14_mean\n",
      "uid_aggregation: uid_id_14_std\n",
      "uid_aggregation: card3_card5_id_14_mean\n",
      "uid_aggregation: card3_card5_id_14_std\n",
      "uid_aggregation: uid_V258_mean\n",
      "uid_aggregation: uid_V258_std\n",
      "uid_aggregation: card3_card5_V258_mean\n",
      "uid_aggregation: card3_card5_V258_std\n",
      "uid_aggregation: uid_V306_mean\n",
      "uid_aggregation: uid_V306_std\n",
      "uid_aggregation: card3_card5_V306_mean\n",
      "uid_aggregation: card3_card5_V306_std\n",
      "uid_aggregation: uid_V307_mean\n",
      "uid_aggregation: uid_V307_std\n",
      "uid_aggregation: card3_card5_V307_mean\n",
      "uid_aggregation: card3_card5_V307_std\n",
      "uid_aggregation: uid_V308_mean\n",
      "uid_aggregation: uid_V308_std\n",
      "uid_aggregation: card3_card5_V308_mean\n",
      "uid_aggregation: card3_card5_V308_std\n",
      "uid_aggregation: uid_V294_mean\n",
      "uid_aggregation: uid_V294_std\n",
      "uid_aggregation: card3_card5_V294_mean\n",
      "uid_aggregation: card3_card5_V294_std\n",
      "timeblock frequency encoding: ProductCD_TransactionAmt_DT_D\n",
      "timeblock frequency encoding: ProductCD_TransactionAmt_DT_W\n",
      "timeblock frequency encoding: ProductCD_cents_DT_D\n",
      "timeblock frequency encoding: ProductCD_cents_DT_W\n",
      "uid_aggregation: card1_C1_mean\n",
      "uid_aggregation: card1_C1_std\n",
      "uid_aggregation: card2_C1_mean\n",
      "uid_aggregation: card2_C1_std\n",
      "uid_aggregation: card3_C1_mean\n",
      "uid_aggregation: card3_C1_std\n",
      "uid_aggregation: card5_C1_mean\n",
      "uid_aggregation: card5_C1_std\n",
      "uid_aggregation: uid_C1_mean\n",
      "uid_aggregation: uid_C1_std\n",
      "uid_aggregation: card3_card5_C1_mean\n",
      "uid_aggregation: card3_card5_C1_std\n",
      "uid_aggregation: card1_C2_mean\n",
      "uid_aggregation: card1_C2_std\n",
      "uid_aggregation: card2_C2_mean\n",
      "uid_aggregation: card2_C2_std\n",
      "uid_aggregation: card3_C2_mean\n",
      "uid_aggregation: card3_C2_std\n",
      "uid_aggregation: card5_C2_mean\n",
      "uid_aggregation: card5_C2_std\n",
      "uid_aggregation: uid_C2_mean\n",
      "uid_aggregation: uid_C2_std\n",
      "uid_aggregation: card3_card5_C2_mean\n",
      "uid_aggregation: card3_card5_C2_std\n",
      "uid_aggregation: card1_C3_mean\n",
      "uid_aggregation: card1_C3_std\n",
      "uid_aggregation: card2_C3_mean\n",
      "uid_aggregation: card2_C3_std\n",
      "uid_aggregation: card3_C3_mean\n",
      "uid_aggregation: card3_C3_std\n",
      "uid_aggregation: card5_C3_mean\n",
      "uid_aggregation: card5_C3_std\n",
      "uid_aggregation: uid_C3_mean\n",
      "uid_aggregation: uid_C3_std\n",
      "uid_aggregation: card3_card5_C3_mean\n",
      "uid_aggregation: card3_card5_C3_std\n",
      "uid_aggregation: card1_C4_mean\n",
      "uid_aggregation: card1_C4_std\n",
      "uid_aggregation: card2_C4_mean\n",
      "uid_aggregation: card2_C4_std\n",
      "uid_aggregation: card3_C4_mean\n",
      "uid_aggregation: card3_C4_std\n",
      "uid_aggregation: card5_C4_mean\n",
      "uid_aggregation: card5_C4_std\n",
      "uid_aggregation: uid_C4_mean\n",
      "uid_aggregation: uid_C4_std\n",
      "uid_aggregation: card3_card5_C4_mean\n",
      "uid_aggregation: card3_card5_C4_std\n",
      "uid_aggregation: card1_C5_mean\n",
      "uid_aggregation: card1_C5_std\n",
      "uid_aggregation: card2_C5_mean\n",
      "uid_aggregation: card2_C5_std\n",
      "uid_aggregation: card3_C5_mean\n",
      "uid_aggregation: card3_C5_std\n",
      "uid_aggregation: card5_C5_mean\n",
      "uid_aggregation: card5_C5_std\n",
      "uid_aggregation: uid_C5_mean\n",
      "uid_aggregation: uid_C5_std\n",
      "uid_aggregation: card3_card5_C5_mean\n",
      "uid_aggregation: card3_card5_C5_std\n",
      "uid_aggregation: card1_C6_mean\n",
      "uid_aggregation: card1_C6_std\n",
      "uid_aggregation: card2_C6_mean\n",
      "uid_aggregation: card2_C6_std\n",
      "uid_aggregation: card3_C6_mean\n",
      "uid_aggregation: card3_C6_std\n",
      "uid_aggregation: card5_C6_mean\n",
      "uid_aggregation: card5_C6_std\n",
      "uid_aggregation: uid_C6_mean\n",
      "uid_aggregation: uid_C6_std\n",
      "uid_aggregation: card3_card5_C6_mean\n",
      "uid_aggregation: card3_card5_C6_std\n",
      "uid_aggregation: card1_C7_mean\n",
      "uid_aggregation: card1_C7_std\n",
      "uid_aggregation: card2_C7_mean\n",
      "uid_aggregation: card2_C7_std\n",
      "uid_aggregation: card3_C7_mean\n",
      "uid_aggregation: card3_C7_std\n",
      "uid_aggregation: card5_C7_mean\n",
      "uid_aggregation: card5_C7_std\n",
      "uid_aggregation: uid_C7_mean\n",
      "uid_aggregation: uid_C7_std\n",
      "uid_aggregation: card3_card5_C7_mean\n",
      "uid_aggregation: card3_card5_C7_std\n",
      "uid_aggregation: card1_C8_mean\n",
      "uid_aggregation: card1_C8_std\n",
      "uid_aggregation: card2_C8_mean\n",
      "uid_aggregation: card2_C8_std\n",
      "uid_aggregation: card3_C8_mean\n",
      "uid_aggregation: card3_C8_std\n",
      "uid_aggregation: card5_C8_mean\n",
      "uid_aggregation: card5_C8_std\n",
      "uid_aggregation: uid_C8_mean\n",
      "uid_aggregation: uid_C8_std\n",
      "uid_aggregation: card3_card5_C8_mean\n",
      "uid_aggregation: card3_card5_C8_std\n",
      "uid_aggregation: card1_C9_mean\n",
      "uid_aggregation: card1_C9_std\n",
      "uid_aggregation: card2_C9_mean\n",
      "uid_aggregation: card2_C9_std\n",
      "uid_aggregation: card3_C9_mean\n",
      "uid_aggregation: card3_C9_std\n",
      "uid_aggregation: card5_C9_mean\n",
      "uid_aggregation: card5_C9_std\n",
      "uid_aggregation: uid_C9_mean\n",
      "uid_aggregation: uid_C9_std\n",
      "uid_aggregation: card3_card5_C9_mean\n",
      "uid_aggregation: card3_card5_C9_std\n",
      "uid_aggregation: card1_C10_mean\n",
      "uid_aggregation: card1_C10_std\n",
      "uid_aggregation: card2_C10_mean\n",
      "uid_aggregation: card2_C10_std\n",
      "uid_aggregation: card3_C10_mean\n",
      "uid_aggregation: card3_C10_std\n",
      "uid_aggregation: card5_C10_mean\n",
      "uid_aggregation: card5_C10_std\n",
      "uid_aggregation: uid_C10_mean\n",
      "uid_aggregation: uid_C10_std\n",
      "uid_aggregation: card3_card5_C10_mean\n",
      "uid_aggregation: card3_card5_C10_std\n",
      "uid_aggregation: card1_C11_mean\n",
      "uid_aggregation: card1_C11_std\n",
      "uid_aggregation: card2_C11_mean\n",
      "uid_aggregation: card2_C11_std\n",
      "uid_aggregation: card3_C11_mean\n",
      "uid_aggregation: card3_C11_std\n",
      "uid_aggregation: card5_C11_mean\n",
      "uid_aggregation: card5_C11_std\n",
      "uid_aggregation: uid_C11_mean\n",
      "uid_aggregation: uid_C11_std\n",
      "uid_aggregation: card3_card5_C11_mean\n",
      "uid_aggregation: card3_card5_C11_std\n",
      "uid_aggregation: card1_C12_mean\n",
      "uid_aggregation: card1_C12_std\n",
      "uid_aggregation: card2_C12_mean\n",
      "uid_aggregation: card2_C12_std\n",
      "uid_aggregation: card3_C12_mean\n",
      "uid_aggregation: card3_C12_std\n",
      "uid_aggregation: card5_C12_mean\n",
      "uid_aggregation: card5_C12_std\n",
      "uid_aggregation: uid_C12_mean\n",
      "uid_aggregation: uid_C12_std\n",
      "uid_aggregation: card3_card5_C12_mean\n",
      "uid_aggregation: card3_card5_C12_std\n",
      "uid_aggregation: card1_C13_mean\n",
      "uid_aggregation: card1_C13_std\n",
      "uid_aggregation: card2_C13_mean\n",
      "uid_aggregation: card2_C13_std\n",
      "uid_aggregation: card3_C13_mean\n",
      "uid_aggregation: card3_C13_std\n",
      "uid_aggregation: card5_C13_mean\n",
      "uid_aggregation: card5_C13_std\n",
      "uid_aggregation: uid_C13_mean\n",
      "uid_aggregation: uid_C13_std\n",
      "uid_aggregation: card3_card5_C13_mean\n",
      "uid_aggregation: card3_card5_C13_std\n",
      "uid_aggregation: card1_C14_mean\n",
      "uid_aggregation: card1_C14_std\n",
      "uid_aggregation: card2_C14_mean\n",
      "uid_aggregation: card2_C14_std\n",
      "uid_aggregation: card3_C14_mean\n",
      "uid_aggregation: card3_C14_std\n",
      "uid_aggregation: card5_C14_mean\n",
      "uid_aggregation: card5_C14_std\n",
      "uid_aggregation: uid_C14_mean\n",
      "uid_aggregation: uid_C14_std\n",
      "uid_aggregation: card3_card5_C14_mean\n",
      "uid_aggregation: card3_card5_C14_std\n",
      "uid_aggregation: card1_dist1_mean\n",
      "uid_aggregation: card1_dist1_std\n",
      "uid_aggregation: card2_dist1_mean\n",
      "uid_aggregation: card2_dist1_std\n",
      "uid_aggregation: card3_dist1_mean\n",
      "uid_aggregation: card3_dist1_std\n",
      "uid_aggregation: card5_dist1_mean\n",
      "uid_aggregation: card5_dist1_std\n",
      "uid_aggregation: uid_dist1_mean\n",
      "uid_aggregation: uid_dist1_std\n",
      "uid_aggregation: card3_card5_dist1_mean\n",
      "uid_aggregation: card3_card5_dist1_std\n",
      "processing nan group agg for: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11']\n",
      "processing nan group agg for: ['V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34']\n",
      "processing nan group agg for: ['V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52']\n",
      "processing nan group agg for: ['V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74']\n",
      "processing nan group agg for: ['V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94']\n",
      "processing nan group agg for: ['V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137']\n",
      "processing nan group agg for: ['V138', 'V139', 'V140', 'V141', 'V142', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V161', 'V162', 'V163']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing nan group agg for: ['V143', 'V144', 'V145', 'V150', 'V151', 'V152', 'V159', 'V160', 'V164', 'V165', 'V166']\n",
      "processing nan group agg for: ['V167', 'V168', 'V172', 'V173', 'V176', 'V177', 'V178', 'V179', 'V181', 'V182', 'V183', 'V186', 'V187', 'V190', 'V191', 'V192', 'V193', 'V196', 'V199', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216']\n",
      "processing nan group agg for: ['V169', 'V170', 'V171', 'V174', 'V175', 'V180', 'V184', 'V185', 'V188', 'V189', 'V194', 'V195', 'V197', 'V198', 'V200', 'V201', 'V208', 'V209', 'V210']\n",
      "processing nan group agg for: ['V217', 'V218', 'V219', 'V223', 'V224', 'V225', 'V226', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V235', 'V236', 'V237', 'V240', 'V241', 'V242', 'V243', 'V244', 'V246', 'V247', 'V248', 'V249', 'V252', 'V253', 'V254', 'V257', 'V258', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278']\n",
      "processing nan group agg for: ['V220', 'V221', 'V222', 'V227', 'V234', 'V238', 'V239', 'V245', 'V250', 'V251', 'V255', 'V256', 'V259', 'V270', 'V271', 'V272']\n",
      "processing nan group agg for: ['V279', 'V280', 'V284', 'V285', 'V286', 'V287', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V297', 'V298', 'V299', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321']\n",
      "processing nan group agg for: ['V281', 'V282', 'V283', 'V288', 'V289', 'V296', 'V300', 'V301', 'V313', 'V314', 'V315']\n",
      "processing nan group agg for: ['V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\n",
      "remove_features: ['TransactionID', 'TransactionDT', 'DT', 'DT_M', 'DT_W', 'DT_D', 'DT_hour', 'DT_day_week', 'DT_day_month', 'DT_D_FE_FULL', 'ProductCD_card1', 'card1_addr1', 'TransactionAmt_dist2', 'uid', 'card3_card5', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'ProductCD_TransactionAmt', 'cents', 'ProductCD_cents', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'DeviceInfo', 'DeviceInfo_device', 'DeviceInfo_version', 'id_30', 'id_30_device', 'id_30_version', 'id_31', 'id_31_device', 'id_33', 'D5_DT_W_std_score', 'ProductCD_TransactionAmt_DT_W', 'D4_DT_D_std_score', 'D15_DT_D_std_score', 'D3_DT_W_std_score', 'D11_DT_W_std_score', 'card3_card5_DT_W_week_day_dist', 'card5_DT_W_week_day_dist', 'D10_DT_D_std_score', 'card3_card5_DT_D', 'ProductCD_cents_DT_D', 'D4_DT_W_std_score', 'D15_DT_W_std_score', 'uid_DT_D', 'card3_DT_W_week_day_dist', 'D10_DT_W_std_score', 'D8_DT_D_std_score', 'card3_card5_DT_W', 'ProductCD_cents_DT_W', 'uid_DT_W', 'D8_DT_W_std_score']\n",
      "train.shape : (472432, 990), test.shape : (118108, 993)\n",
      "Wall time: 16min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_val, category_cols1 = fe1(X_train, X_val)\n",
    "fi_df, best_iteration1, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=category_cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "fi_df, best_iteration1, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=category_cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
